{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec87352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.19.2)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (4.62.3)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "     |████████████████████████████████| 5.8 MB 28.0 MB/s            \n",
      "\u001b[?25hCollecting tensorboardX\n",
      "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
      "     |████████████████████████████████| 125 kB 124.4 MB/s            \n",
      "\u001b[?25hCollecting ml-collections\n",
      "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
      "     |████████████████████████████████| 77 kB 1.2 MB/s              \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting medpy\n",
      "  Downloading MedPy-0.4.0.tar.gz (151 kB)\n",
      "     |████████████████████████████████| 151 kB 116.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting SimpleITK\n",
      "  Downloading SimpleITK-2.1.1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
      "     |████████████████████████████████| 48.4 MB 113.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.5.3)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (3.1.0)\n",
      "Collecting timm\n",
      "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
      "     |████████████████████████████████| 431 kB 119.4 MB/s            \n",
      "\u001b[?25hCollecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 3)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 3)) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 3)) (2.26.0)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "     |████████████████████████████████| 4.3 MB 107.9 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 116.2 MB/s            \n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.2-py2.py3-none-any.whl (156 kB)\n",
      "     |████████████████████████████████| 156 kB 118.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 3)) (3.19.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     |████████████████████████████████| 126 kB 122.5 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 91.7 MB/s            \n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 979 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboardX->-r requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ml-collections->-r requirements.txt (line 5)) (5.4.1)\n",
      "Requirement already satisfied: contextlib2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ml-collections->-r requirements.txt (line 5)) (0.6.0.post1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ml-collections->-r requirements.txt (line 5)) (0.8)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from h5py->-r requirements.txt (line 9)) (1.5.1)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from timm->-r requirements.txt (line 10)) (0.8.2)\n",
      "Requirement already satisfied: torch>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from timm->-r requirements.txt (line 10)) (1.7.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (4.7.2)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 102.7 MB/s            \n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 3)) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 3)) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 3)) (1.26.8)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.4->timm->-r requirements.txt (line 10)) (4.0.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torchvision->timm->-r requirements.txt (line 10)) (8.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 121.9 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: ml-collections, medpy\n",
      "  Building wheel for ml-collections (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94496 sha256=1658a6a47e62947ff256e3ce4615c0b5d25e97748ba56b8938894c26a2ad6328\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/df/01/44/2557139f3718f946cca4de88b0135f11cd3ccf5757af6240b8\n",
      "  Building wheel for medpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for medpy: filename=MedPy-0.4.0-py3-none-any.whl size=214957 sha256=370e12709ca0df8dd5bb960293f04676f82efb978b6ddef7a4b6a6b043d3a582\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/62/5d/88/2816a43ab870de19b56a48413dcd835495dcbab36370fb9020\n",
      "Successfully built ml-collections medpy\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, SimpleITK, markdown, grpcio, google-auth-oauthlib, absl-py, yacs, timm, tensorboardX, tensorboard, ml-collections, medpy, einops\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.7.0\n",
      "    Uninstalling importlib-metadata-3.7.0:\n",
      "      Successfully uninstalled importlib-metadata-3.7.0\n",
      "Successfully installed SimpleITK-2.1.1 absl-py-1.0.0 cachetools-4.2.4 einops-0.4.1 google-auth-2.6.2 google-auth-oauthlib-0.4.6 grpcio-1.44.0 importlib-metadata-4.8.3 markdown-3.3.6 medpy-0.4.0 ml-collections-0.1.1 oauthlib-3.2.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.5 timm-0.5.4 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16edd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from configs/swin_tiny_patch4_window7_224_lite.yaml\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:9\n",
      "---final upsample expand_first---\n",
      "pretrained_path:./pretrained_ckpt/swin_tiny_patch4_window7_224.pth\n",
      "---start load pretrained modle of swin encoder---\n",
      "Namespace(accumulation_steps=None, amp_opt_level='O1', base_lr=0.05, batch_size=24, cache_mode='part', cfg='configs/swin_tiny_patch4_window7_224_lite.yaml', dataset='Synapse', deterministic=1, eval=False, img_size=224, list_dir='./lists/lists_Synapse', max_epochs=50, max_iterations=30000, n_gpu=1, num_classes=9, opts=None, output_dir='output/', resume=None, root_path='../data/Synapse/train_npz', seed=1234, tag=None, throughput=False, use_checkpoint=False, zip=False)\n",
      "The length of train set is: 2211\n",
      "93 iterations per epoch. 4650 max iterations \n",
      "  0%|                                          | 0/50 [00:00<?, ?it/s]iteration 1 : loss : 1.456503, loss_ce: 2.237623\n",
      "iteration 2 : loss : 1.111409, loss_ce: 1.432655\n",
      "iteration 3 : loss : 0.740857, loss_ce: 0.558628\n",
      "iteration 4 : loss : 0.615844, loss_ce: 0.226780\n",
      "iteration 5 : loss : 0.669238, loss_ce: 0.344215\n",
      "iteration 6 : loss : 0.649017, loss_ce: 0.289867\n",
      "iteration 7 : loss : 0.646459, loss_ce: 0.281499\n",
      "iteration 8 : loss : 0.627648, loss_ce: 0.234589\n",
      "iteration 9 : loss : 0.705853, loss_ce: 0.427668\n",
      "iteration 10 : loss : 0.679517, loss_ce: 0.362530\n",
      "iteration 11 : loss : 0.653708, loss_ce: 0.298713\n",
      "iteration 12 : loss : 0.715466, loss_ce: 0.451412\n",
      "iteration 13 : loss : 0.704728, loss_ce: 0.425072\n",
      "iteration 14 : loss : 0.667174, loss_ce: 0.333443\n",
      "iteration 15 : loss : 0.672000, loss_ce: 0.347823\n",
      "iteration 16 : loss : 0.652317, loss_ce: 0.305952\n",
      "iteration 17 : loss : 0.606265, loss_ce: 0.205944\n",
      "iteration 18 : loss : 0.638434, loss_ce: 0.300779\n",
      "iteration 19 : loss : 0.647240, loss_ce: 0.327220\n",
      "iteration 20 : loss : 0.627260, loss_ce: 0.276223\n",
      "iteration 21 : loss : 0.610387, loss_ce: 0.222110\n",
      "iteration 22 : loss : 0.606465, loss_ce: 0.208544\n",
      "iteration 23 : loss : 0.591142, loss_ce: 0.167844\n",
      "iteration 24 : loss : 0.589960, loss_ce: 0.174234\n",
      "iteration 25 : loss : 0.586683, loss_ce: 0.182116\n",
      "iteration 26 : loss : 0.583492, loss_ce: 0.225544\n",
      "iteration 27 : loss : 0.600075, loss_ce: 0.258213\n",
      "iteration 28 : loss : 0.578962, loss_ce: 0.193280\n",
      "iteration 29 : loss : 0.588391, loss_ce: 0.241408\n",
      "iteration 30 : loss : 0.579292, loss_ce: 0.242052\n",
      "iteration 31 : loss : 0.566939, loss_ce: 0.156051\n",
      "iteration 32 : loss : 0.579025, loss_ce: 0.168772\n",
      "iteration 33 : loss : 0.599923, loss_ce: 0.276690\n",
      "iteration 34 : loss : 0.656693, loss_ce: 0.313862\n",
      "iteration 35 : loss : 0.649795, loss_ce: 0.291257\n",
      "iteration 36 : loss : 0.648242, loss_ce: 0.286823\n",
      "iteration 37 : loss : 0.600172, loss_ce: 0.168995\n",
      "iteration 38 : loss : 0.679190, loss_ce: 0.364756\n",
      "iteration 39 : loss : 0.670020, loss_ce: 0.344778\n",
      "iteration 40 : loss : 0.638462, loss_ce: 0.271669\n",
      "iteration 41 : loss : 0.614340, loss_ce: 0.221952\n",
      "iteration 42 : loss : 0.588780, loss_ce: 0.169952\n",
      "iteration 43 : loss : 0.615062, loss_ce: 0.252097\n",
      "iteration 44 : loss : 0.622026, loss_ce: 0.282765\n",
      "iteration 45 : loss : 0.616175, loss_ce: 0.270621\n",
      "iteration 46 : loss : 0.621145, loss_ce: 0.296742\n",
      "iteration 47 : loss : 0.588662, loss_ce: 0.210407\n",
      "iteration 48 : loss : 0.607596, loss_ce: 0.252080\n",
      "iteration 49 : loss : 0.601140, loss_ce: 0.243527\n",
      "iteration 50 : loss : 0.575170, loss_ce: 0.171498\n",
      "iteration 51 : loss : 0.587011, loss_ce: 0.218609\n",
      "iteration 52 : loss : 0.564093, loss_ce: 0.142805\n",
      "iteration 53 : loss : 0.584033, loss_ce: 0.220461\n",
      "iteration 54 : loss : 0.569180, loss_ce: 0.197180\n",
      "iteration 55 : loss : 0.568311, loss_ce: 0.187851\n",
      "iteration 56 : loss : 0.561021, loss_ce: 0.176849\n",
      "iteration 57 : loss : 0.564259, loss_ce: 0.208545\n",
      "iteration 58 : loss : 0.599573, loss_ce: 0.288542\n",
      "iteration 59 : loss : 0.581548, loss_ce: 0.259441\n",
      "iteration 60 : loss : 0.573255, loss_ce: 0.220157\n",
      "iteration 61 : loss : 0.581583, loss_ce: 0.232086\n",
      "iteration 62 : loss : 0.558530, loss_ce: 0.166127\n",
      "iteration 63 : loss : 0.565539, loss_ce: 0.182269\n",
      "iteration 64 : loss : 0.553571, loss_ce: 0.149808\n",
      "iteration 65 : loss : 0.552512, loss_ce: 0.162340\n",
      "iteration 66 : loss : 0.569726, loss_ce: 0.210194\n",
      "iteration 67 : loss : 0.554478, loss_ce: 0.176889\n",
      "iteration 68 : loss : 0.583295, loss_ce: 0.239313\n",
      "iteration 69 : loss : 0.561427, loss_ce: 0.174199\n",
      "iteration 70 : loss : 0.587011, loss_ce: 0.252494\n",
      "iteration 71 : loss : 0.568991, loss_ce: 0.251219\n",
      "iteration 72 : loss : 0.584434, loss_ce: 0.195068\n",
      "iteration 73 : loss : 0.553998, loss_ce: 0.188846\n",
      "iteration 74 : loss : 0.573493, loss_ce: 0.170435\n",
      "iteration 75 : loss : 0.575480, loss_ce: 0.188388\n",
      "iteration 76 : loss : 0.568433, loss_ce: 0.181308\n",
      "iteration 77 : loss : 0.576809, loss_ce: 0.212341\n",
      "iteration 78 : loss : 0.552534, loss_ce: 0.164239\n",
      "iteration 79 : loss : 0.558396, loss_ce: 0.192774\n",
      "iteration 80 : loss : 0.575065, loss_ce: 0.233345\n",
      "iteration 81 : loss : 0.539666, loss_ce: 0.130780\n",
      "iteration 82 : loss : 0.549390, loss_ce: 0.129764\n",
      "iteration 83 : loss : 0.585911, loss_ce: 0.194171\n",
      "iteration 84 : loss : 0.593146, loss_ce: 0.236076\n",
      "iteration 85 : loss : 0.554224, loss_ce: 0.188459\n",
      "iteration 86 : loss : 0.584744, loss_ce: 0.273315\n",
      "iteration 87 : loss : 0.596951, loss_ce: 0.295280\n",
      "iteration 88 : loss : 0.565986, loss_ce: 0.200636\n",
      "iteration 89 : loss : 0.555528, loss_ce: 0.136883\n",
      "iteration 90 : loss : 0.567535, loss_ce: 0.144094\n",
      "iteration 91 : loss : 0.562053, loss_ce: 0.123291\n",
      "iteration 92 : loss : 0.568475, loss_ce: 0.140583\n",
      "iteration 93 : loss : 0.626192, loss_ce: 0.301065\n",
      "  2%|▋                                 | 1/50 [00:57<46:54, 57.44s/it]iteration 94 : loss : 0.546684, loss_ce: 0.158250\n",
      "iteration 95 : loss : 0.548359, loss_ce: 0.194144\n",
      "iteration 96 : loss : 0.580530, loss_ce: 0.268856\n",
      "iteration 97 : loss : 0.580160, loss_ce: 0.242252\n",
      "iteration 98 : loss : 0.567853, loss_ce: 0.201518\n",
      "iteration 99 : loss : 0.548992, loss_ce: 0.166561\n",
      "iteration 100 : loss : 0.549586, loss_ce: 0.164703\n",
      "iteration 101 : loss : 0.550036, loss_ce: 0.184647\n",
      "iteration 102 : loss : 0.546489, loss_ce: 0.183526\n",
      "iteration 103 : loss : 0.542323, loss_ce: 0.181347\n",
      "iteration 104 : loss : 0.530107, loss_ce: 0.131746\n",
      "iteration 105 : loss : 0.533314, loss_ce: 0.134885\n",
      "iteration 106 : loss : 0.547281, loss_ce: 0.144855\n",
      "iteration 107 : loss : 0.530899, loss_ce: 0.127180\n",
      "iteration 108 : loss : 0.532716, loss_ce: 0.154105\n",
      "iteration 109 : loss : 0.532551, loss_ce: 0.159593\n",
      "iteration 110 : loss : 0.547317, loss_ce: 0.217532\n",
      "iteration 111 : loss : 0.521645, loss_ce: 0.114974\n",
      "iteration 112 : loss : 0.514058, loss_ce: 0.120029\n",
      "iteration 113 : loss : 0.519281, loss_ce: 0.138954\n",
      "iteration 114 : loss : 0.519672, loss_ce: 0.162791\n",
      "iteration 115 : loss : 0.515324, loss_ce: 0.110839\n",
      "iteration 116 : loss : 0.519258, loss_ce: 0.095513\n",
      "iteration 117 : loss : 0.517717, loss_ce: 0.135192\n",
      "iteration 118 : loss : 0.504390, loss_ce: 0.123819\n",
      "iteration 119 : loss : 0.538137, loss_ce: 0.201728\n",
      "iteration 120 : loss : 0.521335, loss_ce: 0.166306\n",
      "iteration 121 : loss : 0.511194, loss_ce: 0.127344\n",
      "iteration 122 : loss : 0.546784, loss_ce: 0.220571\n",
      "iteration 123 : loss : 0.494352, loss_ce: 0.104413\n",
      "iteration 124 : loss : 0.522071, loss_ce: 0.177905\n",
      "iteration 125 : loss : 0.511299, loss_ce: 0.114252\n",
      "iteration 126 : loss : 0.536218, loss_ce: 0.182626\n",
      "iteration 127 : loss : 0.527474, loss_ce: 0.182861\n",
      "iteration 128 : loss : 0.639942, loss_ce: 0.275308\n",
      "iteration 129 : loss : 0.611870, loss_ce: 0.224498\n",
      "iteration 130 : loss : 0.552025, loss_ce: 0.136317\n",
      "iteration 131 : loss : 0.572170, loss_ce: 0.264712\n",
      "iteration 132 : loss : 0.589619, loss_ce: 0.323338\n",
      "iteration 133 : loss : 0.606027, loss_ce: 0.325669\n",
      "iteration 134 : loss : 0.546052, loss_ce: 0.171784\n",
      "iteration 135 : loss : 0.585355, loss_ce: 0.223145\n",
      "iteration 136 : loss : 0.578874, loss_ce: 0.175708\n",
      "iteration 137 : loss : 0.576234, loss_ce: 0.150839\n",
      "iteration 138 : loss : 0.599005, loss_ce: 0.254950\n",
      "iteration 139 : loss : 0.535423, loss_ce: 0.138861\n",
      "iteration 140 : loss : 0.568679, loss_ce: 0.241435\n",
      "iteration 141 : loss : 0.561592, loss_ce: 0.226044\n",
      "iteration 142 : loss : 0.525490, loss_ce: 0.121671\n",
      "iteration 143 : loss : 0.549712, loss_ce: 0.155073\n",
      "iteration 144 : loss : 0.560853, loss_ce: 0.160705\n",
      "iteration 145 : loss : 0.534408, loss_ce: 0.120647\n",
      "iteration 146 : loss : 0.567176, loss_ce: 0.224671\n",
      "iteration 147 : loss : 0.523820, loss_ce: 0.139252\n",
      "iteration 148 : loss : 0.528272, loss_ce: 0.154023\n",
      "iteration 149 : loss : 0.545734, loss_ce: 0.202549\n",
      "iteration 150 : loss : 0.520754, loss_ce: 0.151088\n",
      "iteration 151 : loss : 0.556635, loss_ce: 0.174405\n",
      "iteration 152 : loss : 0.547759, loss_ce: 0.172972\n",
      "iteration 153 : loss : 0.521587, loss_ce: 0.147765\n",
      "iteration 154 : loss : 0.514005, loss_ce: 0.099377\n",
      "iteration 155 : loss : 0.518305, loss_ce: 0.151062\n",
      "iteration 156 : loss : 0.520271, loss_ce: 0.146602\n",
      "iteration 157 : loss : 0.504297, loss_ce: 0.123198\n",
      "iteration 158 : loss : 0.516561, loss_ce: 0.102806\n",
      "iteration 159 : loss : 0.518668, loss_ce: 0.164817\n",
      "iteration 160 : loss : 0.527321, loss_ce: 0.183706\n",
      "iteration 161 : loss : 0.523716, loss_ce: 0.151449\n",
      "iteration 162 : loss : 0.525134, loss_ce: 0.164536\n",
      "iteration 163 : loss : 0.512466, loss_ce: 0.145402\n",
      "iteration 164 : loss : 0.504684, loss_ce: 0.148936\n",
      "iteration 165 : loss : 0.513176, loss_ce: 0.161164\n",
      "iteration 166 : loss : 0.508332, loss_ce: 0.147716\n",
      "iteration 167 : loss : 0.524891, loss_ce: 0.189949\n",
      "iteration 168 : loss : 0.499025, loss_ce: 0.117906\n",
      "iteration 169 : loss : 0.517397, loss_ce: 0.182852\n",
      "iteration 170 : loss : 0.513706, loss_ce: 0.163362\n",
      "iteration 171 : loss : 0.493078, loss_ce: 0.125392\n",
      "iteration 172 : loss : 0.536444, loss_ce: 0.186752\n",
      "iteration 173 : loss : 0.513318, loss_ce: 0.155374\n",
      "iteration 174 : loss : 0.502037, loss_ce: 0.134315\n",
      "iteration 175 : loss : 0.531789, loss_ce: 0.228152\n",
      "iteration 176 : loss : 0.519430, loss_ce: 0.191041\n",
      "iteration 177 : loss : 0.524334, loss_ce: 0.167329\n",
      "iteration 178 : loss : 0.501373, loss_ce: 0.098331\n",
      "iteration 179 : loss : 0.524935, loss_ce: 0.183509\n",
      "iteration 180 : loss : 0.498014, loss_ce: 0.123149\n",
      "iteration 181 : loss : 0.494447, loss_ce: 0.110034\n",
      "iteration 182 : loss : 0.524817, loss_ce: 0.163085\n",
      "iteration 183 : loss : 0.508075, loss_ce: 0.120492\n",
      "iteration 184 : loss : 0.519168, loss_ce: 0.164948\n",
      "iteration 185 : loss : 0.521295, loss_ce: 0.182440\n",
      "iteration 186 : loss : 0.540648, loss_ce: 0.043644\n",
      "  4%|█▎                                | 2/50 [01:54<45:48, 57.26s/it]iteration 187 : loss : 0.525522, loss_ce: 0.122305\n",
      "iteration 188 : loss : 0.489443, loss_ce: 0.097826\n",
      "iteration 189 : loss : 0.510714, loss_ce: 0.161599\n",
      "iteration 190 : loss : 0.500976, loss_ce: 0.133299\n",
      "iteration 191 : loss : 0.485573, loss_ce: 0.116387\n",
      "iteration 192 : loss : 0.491888, loss_ce: 0.105415\n",
      "iteration 193 : loss : 0.516487, loss_ce: 0.114297\n",
      "iteration 194 : loss : 0.516300, loss_ce: 0.188939\n",
      "iteration 195 : loss : 0.513899, loss_ce: 0.184903\n",
      "iteration 196 : loss : 0.501658, loss_ce: 0.156081\n",
      "iteration 197 : loss : 0.557668, loss_ce: 0.131910\n",
      "iteration 198 : loss : 0.513326, loss_ce: 0.149903\n",
      "iteration 199 : loss : 0.500362, loss_ce: 0.158746\n",
      "iteration 200 : loss : 0.499178, loss_ce: 0.159789\n",
      "iteration 201 : loss : 0.508108, loss_ce: 0.168983\n",
      "iteration 202 : loss : 0.501505, loss_ce: 0.135614\n",
      "iteration 203 : loss : 0.491946, loss_ce: 0.132366\n",
      "iteration 204 : loss : 0.487528, loss_ce: 0.111453\n",
      "iteration 205 : loss : 0.492289, loss_ce: 0.134858\n",
      "iteration 206 : loss : 0.522960, loss_ce: 0.204140\n",
      "iteration 207 : loss : 0.492032, loss_ce: 0.138987\n",
      "iteration 208 : loss : 0.476822, loss_ce: 0.093199\n",
      "iteration 209 : loss : 0.480039, loss_ce: 0.122901\n",
      "iteration 210 : loss : 0.493458, loss_ce: 0.098708\n",
      "iteration 211 : loss : 0.477344, loss_ce: 0.101621\n",
      "iteration 212 : loss : 0.507638, loss_ce: 0.158540\n",
      "iteration 213 : loss : 0.484117, loss_ce: 0.094967\n",
      "iteration 214 : loss : 0.499099, loss_ce: 0.162569\n",
      "iteration 215 : loss : 0.481320, loss_ce: 0.129341\n",
      "iteration 216 : loss : 0.497328, loss_ce: 0.113067\n",
      "iteration 217 : loss : 0.498535, loss_ce: 0.159717\n",
      "iteration 218 : loss : 0.502714, loss_ce: 0.186746\n",
      "iteration 219 : loss : 0.497556, loss_ce: 0.165692\n",
      "iteration 220 : loss : 0.498699, loss_ce: 0.165806\n",
      "iteration 221 : loss : 0.488752, loss_ce: 0.164008\n",
      "iteration 222 : loss : 0.506574, loss_ce: 0.150889\n",
      "iteration 223 : loss : 0.497621, loss_ce: 0.159212\n",
      "iteration 224 : loss : 0.478721, loss_ce: 0.140980\n",
      "iteration 225 : loss : 0.482004, loss_ce: 0.133589\n",
      "iteration 226 : loss : 0.469154, loss_ce: 0.097262\n",
      "iteration 227 : loss : 0.474818, loss_ce: 0.106774\n",
      "iteration 228 : loss : 0.481075, loss_ce: 0.100172\n",
      "iteration 229 : loss : 0.478037, loss_ce: 0.113001\n",
      "iteration 230 : loss : 0.471191, loss_ce: 0.099132\n",
      "iteration 231 : loss : 0.491421, loss_ce: 0.157440\n",
      "iteration 232 : loss : 0.476241, loss_ce: 0.103324\n",
      "iteration 233 : loss : 0.493922, loss_ce: 0.133811\n",
      "iteration 234 : loss : 0.498664, loss_ce: 0.198422\n",
      "iteration 235 : loss : 0.487632, loss_ce: 0.112096\n",
      "iteration 236 : loss : 0.516403, loss_ce: 0.180681\n",
      "iteration 237 : loss : 0.490598, loss_ce: 0.140410\n",
      "iteration 238 : loss : 0.487952, loss_ce: 0.146924\n",
      "iteration 239 : loss : 0.503201, loss_ce: 0.198567\n",
      "iteration 240 : loss : 0.472643, loss_ce: 0.147421\n",
      "iteration 241 : loss : 0.499419, loss_ce: 0.155057\n",
      "iteration 242 : loss : 0.495497, loss_ce: 0.135606\n",
      "iteration 243 : loss : 0.454700, loss_ce: 0.098524\n",
      "iteration 244 : loss : 0.542600, loss_ce: 0.260294\n",
      "iteration 245 : loss : 0.471484, loss_ce: 0.129945\n",
      "iteration 246 : loss : 0.465927, loss_ce: 0.094533\n",
      "iteration 247 : loss : 0.501184, loss_ce: 0.123093\n",
      "iteration 248 : loss : 0.491726, loss_ce: 0.134569\n",
      "iteration 249 : loss : 0.493696, loss_ce: 0.158113\n",
      "iteration 250 : loss : 0.506422, loss_ce: 0.209173\n",
      "iteration 251 : loss : 0.521603, loss_ce: 0.236868\n",
      "iteration 252 : loss : 0.474942, loss_ce: 0.128682\n",
      "iteration 253 : loss : 0.474011, loss_ce: 0.107091\n",
      "iteration 254 : loss : 0.497944, loss_ce: 0.128164\n",
      "iteration 255 : loss : 0.524583, loss_ce: 0.217222\n",
      "iteration 256 : loss : 0.464554, loss_ce: 0.139837\n",
      "iteration 257 : loss : 0.499359, loss_ce: 0.175852\n",
      "iteration 258 : loss : 0.465152, loss_ce: 0.077393\n",
      "iteration 259 : loss : 0.556090, loss_ce: 0.241895\n",
      "iteration 260 : loss : 0.508918, loss_ce: 0.181899\n",
      "iteration 261 : loss : 0.497820, loss_ce: 0.195545\n",
      "iteration 262 : loss : 0.488476, loss_ce: 0.129293\n",
      "iteration 263 : loss : 0.485710, loss_ce: 0.101655\n",
      "iteration 264 : loss : 0.491925, loss_ce: 0.118006\n",
      "iteration 265 : loss : 0.509151, loss_ce: 0.193889\n",
      "iteration 266 : loss : 0.498607, loss_ce: 0.186453\n",
      "iteration 267 : loss : 0.509079, loss_ce: 0.207810\n",
      "iteration 268 : loss : 0.494903, loss_ce: 0.154580\n",
      "iteration 269 : loss : 0.496610, loss_ce: 0.145603\n",
      "iteration 270 : loss : 0.515872, loss_ce: 0.145016\n",
      "iteration 271 : loss : 0.471878, loss_ce: 0.099271\n",
      "iteration 272 : loss : 0.481702, loss_ce: 0.144046\n",
      "iteration 273 : loss : 0.493856, loss_ce: 0.193527\n",
      "iteration 274 : loss : 0.480365, loss_ce: 0.132431\n",
      "iteration 275 : loss : 0.465728, loss_ce: 0.116339\n",
      "iteration 276 : loss : 0.515036, loss_ce: 0.183954\n",
      "iteration 277 : loss : 0.491846, loss_ce: 0.169142\n",
      "iteration 278 : loss : 0.490728, loss_ce: 0.173503\n",
      "iteration 279 : loss : 0.515735, loss_ce: 0.216658\n",
      "  6%|██                                | 3/50 [02:51<44:53, 57.31s/it]iteration 280 : loss : 0.462245, loss_ce: 0.144357\n",
      "iteration 281 : loss : 0.462953, loss_ce: 0.106402\n",
      "iteration 282 : loss : 0.484926, loss_ce: 0.122488\n",
      "iteration 283 : loss : 0.470103, loss_ce: 0.166262\n",
      "iteration 284 : loss : 0.494359, loss_ce: 0.198322\n",
      "iteration 285 : loss : 0.468651, loss_ce: 0.138655\n",
      "iteration 286 : loss : 0.453906, loss_ce: 0.117846\n",
      "iteration 287 : loss : 0.479439, loss_ce: 0.154364\n",
      "iteration 288 : loss : 0.477253, loss_ce: 0.143839\n",
      "iteration 289 : loss : 0.468002, loss_ce: 0.099629\n",
      "iteration 290 : loss : 0.474577, loss_ce: 0.144075\n",
      "iteration 291 : loss : 0.441957, loss_ce: 0.092304\n",
      "iteration 292 : loss : 0.462877, loss_ce: 0.128399\n",
      "iteration 293 : loss : 0.482886, loss_ce: 0.134662\n",
      "iteration 294 : loss : 0.458876, loss_ce: 0.101749\n",
      "iteration 295 : loss : 0.462953, loss_ce: 0.146141\n",
      "iteration 296 : loss : 0.453496, loss_ce: 0.095141\n",
      "iteration 297 : loss : 0.469409, loss_ce: 0.107549\n",
      "iteration 298 : loss : 0.453219, loss_ce: 0.112273\n",
      "iteration 299 : loss : 0.462497, loss_ce: 0.149048\n",
      "iteration 300 : loss : 0.445806, loss_ce: 0.113603\n",
      "iteration 301 : loss : 0.454461, loss_ce: 0.093861\n",
      "iteration 302 : loss : 0.489264, loss_ce: 0.152263\n",
      "iteration 303 : loss : 0.441699, loss_ce: 0.104419\n",
      "iteration 304 : loss : 0.496771, loss_ce: 0.225284\n",
      "iteration 305 : loss : 0.448891, loss_ce: 0.129936\n",
      "iteration 306 : loss : 0.496660, loss_ce: 0.161328\n",
      "iteration 307 : loss : 0.458859, loss_ce: 0.110985\n",
      "iteration 308 : loss : 0.442928, loss_ce: 0.132631\n",
      "iteration 309 : loss : 0.471236, loss_ce: 0.168004\n",
      "iteration 310 : loss : 0.446676, loss_ce: 0.101225\n",
      "iteration 311 : loss : 0.468576, loss_ce: 0.113313\n",
      "iteration 312 : loss : 0.427330, loss_ce: 0.069886\n",
      "iteration 313 : loss : 0.445506, loss_ce: 0.132433\n",
      "iteration 314 : loss : 0.422458, loss_ce: 0.084252\n",
      "iteration 315 : loss : 0.451033, loss_ce: 0.164091\n",
      "iteration 316 : loss : 0.445350, loss_ce: 0.114649\n",
      "iteration 317 : loss : 0.451374, loss_ce: 0.140824\n",
      "iteration 318 : loss : 0.441989, loss_ce: 0.099636\n",
      "iteration 319 : loss : 0.422622, loss_ce: 0.098371\n",
      "iteration 320 : loss : 0.441627, loss_ce: 0.099744\n",
      "iteration 321 : loss : 0.452172, loss_ce: 0.150357\n",
      "iteration 322 : loss : 0.464941, loss_ce: 0.199010\n",
      "iteration 323 : loss : 0.428320, loss_ce: 0.077130\n",
      "iteration 324 : loss : 0.464265, loss_ce: 0.086159\n",
      "iteration 325 : loss : 0.513717, loss_ce: 0.132776\n",
      "iteration 326 : loss : 0.456193, loss_ce: 0.107917\n",
      "iteration 327 : loss : 0.448338, loss_ce: 0.142808\n",
      "iteration 328 : loss : 0.485164, loss_ce: 0.221707\n",
      "iteration 329 : loss : 0.420299, loss_ce: 0.063059\n",
      "iteration 330 : loss : 0.460698, loss_ce: 0.135420\n",
      "iteration 331 : loss : 0.469193, loss_ce: 0.105958\n",
      "iteration 332 : loss : 0.453007, loss_ce: 0.135536\n",
      "iteration 333 : loss : 0.439129, loss_ce: 0.102074\n",
      "iteration 334 : loss : 0.425039, loss_ce: 0.123414\n",
      "iteration 335 : loss : 0.445332, loss_ce: 0.106744\n",
      "iteration 336 : loss : 0.455619, loss_ce: 0.136052\n",
      "iteration 337 : loss : 0.446272, loss_ce: 0.151778\n",
      "iteration 338 : loss : 0.437626, loss_ce: 0.145427\n",
      "iteration 339 : loss : 0.436120, loss_ce: 0.110009\n",
      "iteration 340 : loss : 0.428389, loss_ce: 0.106797\n",
      "iteration 341 : loss : 0.417187, loss_ce: 0.104473\n",
      "iteration 342 : loss : 0.454788, loss_ce: 0.190685\n",
      "iteration 343 : loss : 0.456375, loss_ce: 0.160460\n",
      "iteration 344 : loss : 0.413529, loss_ce: 0.080067\n",
      "iteration 345 : loss : 0.431264, loss_ce: 0.118422\n",
      "iteration 346 : loss : 0.454419, loss_ce: 0.160711\n",
      "iteration 347 : loss : 0.417789, loss_ce: 0.089628\n",
      "iteration 348 : loss : 0.443545, loss_ce: 0.137648\n",
      "iteration 349 : loss : 0.410623, loss_ce: 0.099880\n",
      "iteration 350 : loss : 0.432563, loss_ce: 0.083498\n",
      "iteration 351 : loss : 0.431574, loss_ce: 0.136808\n",
      "iteration 352 : loss : 0.448095, loss_ce: 0.159758\n",
      "iteration 353 : loss : 0.401286, loss_ce: 0.055348\n",
      "iteration 354 : loss : 0.410375, loss_ce: 0.095071\n",
      "iteration 355 : loss : 0.449846, loss_ce: 0.114000\n",
      "iteration 356 : loss : 0.447800, loss_ce: 0.160191\n",
      "iteration 357 : loss : 0.482508, loss_ce: 0.215556\n",
      "iteration 358 : loss : 0.424702, loss_ce: 0.129289\n",
      "iteration 359 : loss : 0.523753, loss_ce: 0.226917\n",
      "iteration 360 : loss : 0.442349, loss_ce: 0.087852\n",
      "iteration 361 : loss : 0.438112, loss_ce: 0.133045\n",
      "iteration 362 : loss : 0.441043, loss_ce: 0.155615\n",
      "iteration 363 : loss : 0.440371, loss_ce: 0.142438\n",
      "iteration 364 : loss : 0.404122, loss_ce: 0.062448\n",
      "iteration 365 : loss : 0.419911, loss_ce: 0.117967\n",
      "iteration 366 : loss : 0.459020, loss_ce: 0.173248\n",
      "iteration 367 : loss : 0.444690, loss_ce: 0.170271\n",
      "iteration 368 : loss : 0.409098, loss_ce: 0.114858\n",
      "iteration 369 : loss : 0.432275, loss_ce: 0.096636\n",
      "iteration 370 : loss : 0.437771, loss_ce: 0.124402\n",
      "iteration 371 : loss : 0.400268, loss_ce: 0.097761\n",
      "iteration 372 : loss : 0.488476, loss_ce: 0.038241\n",
      "  8%|██▋                               | 4/50 [03:49<44:09, 57.59s/it]iteration 373 : loss : 0.423974, loss_ce: 0.116540\n",
      "iteration 374 : loss : 0.414112, loss_ce: 0.112383\n",
      "iteration 375 : loss : 0.423191, loss_ce: 0.106136\n",
      "iteration 376 : loss : 0.413574, loss_ce: 0.130787\n",
      "iteration 377 : loss : 0.447747, loss_ce: 0.088417\n",
      "iteration 378 : loss : 0.449314, loss_ce: 0.148297\n",
      "iteration 379 : loss : 0.445892, loss_ce: 0.152489\n",
      "iteration 380 : loss : 0.442620, loss_ce: 0.100980\n",
      "iteration 381 : loss : 0.415555, loss_ce: 0.118738\n",
      "iteration 382 : loss : 0.429133, loss_ce: 0.132727\n",
      "iteration 383 : loss : 0.439071, loss_ce: 0.105162\n",
      "iteration 384 : loss : 0.460640, loss_ce: 0.206160\n",
      "iteration 385 : loss : 0.405906, loss_ce: 0.105829\n",
      "iteration 386 : loss : 0.427565, loss_ce: 0.127716\n",
      "iteration 387 : loss : 0.397577, loss_ce: 0.104912\n",
      "iteration 388 : loss : 0.442028, loss_ce: 0.156736\n",
      "iteration 389 : loss : 0.427285, loss_ce: 0.127226\n",
      "iteration 390 : loss : 0.433921, loss_ce: 0.130114\n",
      "iteration 391 : loss : 0.381898, loss_ce: 0.035714\n",
      "iteration 392 : loss : 0.409477, loss_ce: 0.083111\n",
      "iteration 393 : loss : 0.403568, loss_ce: 0.142638\n",
      "iteration 394 : loss : 0.422124, loss_ce: 0.127368\n",
      "iteration 395 : loss : 0.399275, loss_ce: 0.116680\n",
      "iteration 396 : loss : 0.375905, loss_ce: 0.107725\n",
      "iteration 397 : loss : 0.392648, loss_ce: 0.120272\n",
      "iteration 398 : loss : 0.388217, loss_ce: 0.092027\n",
      "iteration 399 : loss : 0.426869, loss_ce: 0.085478\n",
      "iteration 400 : loss : 0.387613, loss_ce: 0.102270\n",
      "iteration 401 : loss : 0.407814, loss_ce: 0.131879\n",
      "iteration 402 : loss : 0.398580, loss_ce: 0.121443\n",
      "iteration 403 : loss : 0.370151, loss_ce: 0.073878\n",
      "iteration 404 : loss : 0.418730, loss_ce: 0.063648\n",
      "iteration 405 : loss : 0.374058, loss_ce: 0.101242\n",
      "iteration 406 : loss : 0.372537, loss_ce: 0.110341\n",
      "iteration 407 : loss : 0.356311, loss_ce: 0.074037\n",
      "iteration 408 : loss : 0.377952, loss_ce: 0.079038\n",
      "iteration 409 : loss : 0.445869, loss_ce: 0.109895\n",
      "iteration 410 : loss : 0.366894, loss_ce: 0.088399\n",
      "iteration 411 : loss : 0.407435, loss_ce: 0.141311\n",
      "iteration 412 : loss : 0.421366, loss_ce: 0.131164\n",
      "iteration 413 : loss : 0.409634, loss_ce: 0.104512\n",
      "iteration 414 : loss : 0.390537, loss_ce: 0.102532\n",
      "iteration 415 : loss : 0.354172, loss_ce: 0.108319\n",
      "iteration 416 : loss : 0.440866, loss_ce: 0.218315\n",
      "iteration 417 : loss : 0.397964, loss_ce: 0.108311\n",
      "iteration 418 : loss : 0.399331, loss_ce: 0.131517\n",
      "iteration 419 : loss : 0.375170, loss_ce: 0.122426\n",
      "iteration 420 : loss : 0.381199, loss_ce: 0.105675\n",
      "iteration 421 : loss : 0.395763, loss_ce: 0.057349\n",
      "iteration 422 : loss : 0.398849, loss_ce: 0.051060\n",
      "iteration 423 : loss : 0.394788, loss_ce: 0.102556\n",
      "iteration 424 : loss : 0.382331, loss_ce: 0.113154\n",
      "iteration 425 : loss : 0.384180, loss_ce: 0.119571\n",
      "iteration 426 : loss : 0.457364, loss_ce: 0.151657\n",
      "iteration 427 : loss : 0.409822, loss_ce: 0.094295\n",
      "iteration 428 : loss : 0.438928, loss_ce: 0.161961\n",
      "iteration 429 : loss : 0.429137, loss_ce: 0.201214\n",
      "iteration 430 : loss : 0.418673, loss_ce: 0.126067\n",
      "iteration 431 : loss : 0.461762, loss_ce: 0.085956\n",
      "iteration 432 : loss : 0.491259, loss_ce: 0.142155\n",
      "iteration 433 : loss : 0.416144, loss_ce: 0.139595\n",
      "iteration 434 : loss : 0.441496, loss_ce: 0.091004\n",
      "iteration 435 : loss : 0.439506, loss_ce: 0.194513\n",
      "iteration 436 : loss : 0.433730, loss_ce: 0.182832\n",
      "iteration 437 : loss : 0.419666, loss_ce: 0.127272\n",
      "iteration 438 : loss : 0.432821, loss_ce: 0.130205\n",
      "iteration 439 : loss : 0.418538, loss_ce: 0.117941\n",
      "iteration 440 : loss : 0.370675, loss_ce: 0.090026\n",
      "iteration 441 : loss : 0.364906, loss_ce: 0.108766\n",
      "iteration 442 : loss : 0.366739, loss_ce: 0.099706\n",
      "iteration 443 : loss : 0.394388, loss_ce: 0.123439\n",
      "iteration 444 : loss : 0.375668, loss_ce: 0.124222\n",
      "iteration 445 : loss : 0.363237, loss_ce: 0.091734\n",
      "iteration 446 : loss : 0.378280, loss_ce: 0.122086\n",
      "iteration 447 : loss : 0.355714, loss_ce: 0.067019\n",
      "iteration 448 : loss : 0.356952, loss_ce: 0.106397\n",
      "iteration 449 : loss : 0.393255, loss_ce: 0.105430\n",
      "iteration 450 : loss : 0.417565, loss_ce: 0.092174\n",
      "iteration 451 : loss : 0.389156, loss_ce: 0.091029\n",
      "iteration 452 : loss : 0.415099, loss_ce: 0.180803\n",
      "iteration 453 : loss : 0.432946, loss_ce: 0.179941\n",
      "iteration 454 : loss : 0.391562, loss_ce: 0.154414\n",
      "iteration 455 : loss : 0.367994, loss_ce: 0.111877\n",
      "iteration 456 : loss : 0.436514, loss_ce: 0.157798\n",
      "iteration 457 : loss : 0.380159, loss_ce: 0.086962\n",
      "iteration 458 : loss : 0.358329, loss_ce: 0.107929\n",
      "iteration 459 : loss : 0.370621, loss_ce: 0.145211\n",
      "iteration 460 : loss : 0.386194, loss_ce: 0.102797\n",
      "iteration 461 : loss : 0.361215, loss_ce: 0.056970\n",
      "iteration 462 : loss : 0.365487, loss_ce: 0.107702\n",
      "iteration 463 : loss : 0.341084, loss_ce: 0.095946\n",
      "iteration 464 : loss : 0.321370, loss_ce: 0.060992\n",
      "iteration 465 : loss : 0.501346, loss_ce: 0.137096\n",
      " 10%|███▍                              | 5/50 [04:49<43:38, 58.18s/it]iteration 466 : loss : 0.392580, loss_ce: 0.097150\n",
      "iteration 467 : loss : 0.376514, loss_ce: 0.098687\n",
      "iteration 468 : loss : 0.361833, loss_ce: 0.103544\n",
      "iteration 469 : loss : 0.350289, loss_ce: 0.088938\n",
      "iteration 470 : loss : 0.357277, loss_ce: 0.134669\n",
      "iteration 471 : loss : 0.373953, loss_ce: 0.096327\n",
      "iteration 472 : loss : 0.360822, loss_ce: 0.102107\n",
      "iteration 473 : loss : 0.354027, loss_ce: 0.117846\n",
      "iteration 474 : loss : 0.365358, loss_ce: 0.134202\n",
      "iteration 475 : loss : 0.359711, loss_ce: 0.078571\n",
      "iteration 476 : loss : 0.360470, loss_ce: 0.103870\n",
      "iteration 477 : loss : 0.353892, loss_ce: 0.058256\n",
      "iteration 478 : loss : 0.347190, loss_ce: 0.091164\n",
      "iteration 479 : loss : 0.379309, loss_ce: 0.159598\n",
      "iteration 480 : loss : 0.368782, loss_ce: 0.101926\n",
      "iteration 481 : loss : 0.336827, loss_ce: 0.110450\n",
      "iteration 482 : loss : 0.314888, loss_ce: 0.081083\n",
      "iteration 483 : loss : 0.376776, loss_ce: 0.167520\n",
      "iteration 484 : loss : 0.323758, loss_ce: 0.057565\n",
      "iteration 485 : loss : 0.323664, loss_ce: 0.086416\n",
      "iteration 486 : loss : 0.347266, loss_ce: 0.106431\n",
      "iteration 487 : loss : 0.335681, loss_ce: 0.106936\n",
      "iteration 488 : loss : 0.344345, loss_ce: 0.084198\n",
      "iteration 489 : loss : 0.360107, loss_ce: 0.072971\n",
      "iteration 490 : loss : 0.338801, loss_ce: 0.106962\n",
      "iteration 491 : loss : 0.343991, loss_ce: 0.085937\n",
      "iteration 492 : loss : 0.313782, loss_ce: 0.062839\n",
      "iteration 493 : loss : 0.365310, loss_ce: 0.143696\n",
      "iteration 494 : loss : 0.329452, loss_ce: 0.084799\n",
      "iteration 495 : loss : 0.350254, loss_ce: 0.088796\n",
      "iteration 496 : loss : 0.348002, loss_ce: 0.145673\n",
      "iteration 497 : loss : 0.354411, loss_ce: 0.101530\n",
      "iteration 498 : loss : 0.357710, loss_ce: 0.086638\n",
      "iteration 499 : loss : 0.353177, loss_ce: 0.114339\n",
      "iteration 500 : loss : 0.352935, loss_ce: 0.047095\n",
      "iteration 501 : loss : 0.359298, loss_ce: 0.075220\n",
      "iteration 502 : loss : 0.489897, loss_ce: 0.071194\n",
      "iteration 503 : loss : 0.398786, loss_ce: 0.214824\n",
      "iteration 504 : loss : 0.358409, loss_ce: 0.075586\n",
      "iteration 505 : loss : 0.381831, loss_ce: 0.106274\n",
      "iteration 506 : loss : 0.333788, loss_ce: 0.080959\n",
      "iteration 507 : loss : 0.374175, loss_ce: 0.069275\n",
      "iteration 508 : loss : 0.350845, loss_ce: 0.073104\n",
      "iteration 509 : loss : 0.339546, loss_ce: 0.147846\n",
      "iteration 510 : loss : 0.314398, loss_ce: 0.083057\n",
      "iteration 511 : loss : 0.348435, loss_ce: 0.144000\n",
      "iteration 512 : loss : 0.365895, loss_ce: 0.049484\n",
      "iteration 513 : loss : 0.428541, loss_ce: 0.080239\n",
      "iteration 514 : loss : 0.329390, loss_ce: 0.076464\n",
      "iteration 515 : loss : 0.325038, loss_ce: 0.093064\n",
      "iteration 516 : loss : 0.373984, loss_ce: 0.059303\n",
      "iteration 517 : loss : 0.324997, loss_ce: 0.111987\n",
      "iteration 518 : loss : 0.308552, loss_ce: 0.076174\n",
      "iteration 519 : loss : 0.345386, loss_ce: 0.105795\n",
      "iteration 520 : loss : 0.312494, loss_ce: 0.088299\n",
      "iteration 521 : loss : 0.354111, loss_ce: 0.121236\n",
      "iteration 522 : loss : 0.329495, loss_ce: 0.058937\n",
      "iteration 523 : loss : 0.311764, loss_ce: 0.073170\n",
      "iteration 524 : loss : 0.316412, loss_ce: 0.082148\n",
      "iteration 525 : loss : 0.304321, loss_ce: 0.077974\n",
      "iteration 526 : loss : 0.339106, loss_ce: 0.110272\n",
      "iteration 527 : loss : 0.308259, loss_ce: 0.087342\n",
      "iteration 528 : loss : 0.382061, loss_ce: 0.096312\n",
      "iteration 529 : loss : 0.354940, loss_ce: 0.117744\n",
      "iteration 530 : loss : 0.372402, loss_ce: 0.080913\n",
      "iteration 531 : loss : 0.342609, loss_ce: 0.137556\n",
      "iteration 532 : loss : 0.364573, loss_ce: 0.143071\n",
      "iteration 533 : loss : 0.311880, loss_ce: 0.078460\n",
      "iteration 534 : loss : 0.383588, loss_ce: 0.109738\n",
      "iteration 535 : loss : 0.320222, loss_ce: 0.051752\n",
      "iteration 536 : loss : 0.254849, loss_ce: 0.052545\n",
      "iteration 537 : loss : 0.264225, loss_ce: 0.058907\n",
      "iteration 538 : loss : 0.327037, loss_ce: 0.124650\n",
      "iteration 539 : loss : 0.315318, loss_ce: 0.090543\n",
      "iteration 540 : loss : 0.286678, loss_ce: 0.113656\n",
      "iteration 541 : loss : 0.338753, loss_ce: 0.113625\n",
      "iteration 542 : loss : 0.277639, loss_ce: 0.078331\n",
      "iteration 543 : loss : 0.330368, loss_ce: 0.093919\n",
      "iteration 544 : loss : 0.371989, loss_ce: 0.115817\n",
      "iteration 545 : loss : 0.272095, loss_ce: 0.081303\n",
      "iteration 546 : loss : 0.293950, loss_ce: 0.078223\n",
      "iteration 547 : loss : 0.322539, loss_ce: 0.100676\n",
      "iteration 548 : loss : 0.300046, loss_ce: 0.092513\n",
      "iteration 549 : loss : 0.276110, loss_ce: 0.060714\n",
      "iteration 550 : loss : 0.314086, loss_ce: 0.099834\n",
      "iteration 551 : loss : 0.323854, loss_ce: 0.122802\n",
      "iteration 552 : loss : 0.343472, loss_ce: 0.131463\n",
      "iteration 553 : loss : 0.323763, loss_ce: 0.087502\n",
      "iteration 554 : loss : 0.317111, loss_ce: 0.098102\n",
      "iteration 555 : loss : 0.364769, loss_ce: 0.108779\n",
      "iteration 556 : loss : 0.306161, loss_ce: 0.089968\n",
      "iteration 557 : loss : 0.349817, loss_ce: 0.120694\n",
      "iteration 558 : loss : 0.494359, loss_ce: 0.017402\n",
      " 12%|████                              | 6/50 [05:50<43:21, 59.12s/it]iteration 559 : loss : 0.281497, loss_ce: 0.111845\n",
      "iteration 560 : loss : 0.322303, loss_ce: 0.138473\n",
      "iteration 561 : loss : 0.298012, loss_ce: 0.084629\n",
      "iteration 562 : loss : 0.347603, loss_ce: 0.083644\n",
      "iteration 563 : loss : 0.335734, loss_ce: 0.093474\n",
      "iteration 564 : loss : 0.384746, loss_ce: 0.120064\n",
      "iteration 565 : loss : 0.327724, loss_ce: 0.114267\n",
      "iteration 566 : loss : 0.312575, loss_ce: 0.073758\n",
      "iteration 567 : loss : 0.313327, loss_ce: 0.080941\n",
      "iteration 568 : loss : 0.279635, loss_ce: 0.107930\n",
      "iteration 569 : loss : 0.236039, loss_ce: 0.064868\n",
      "iteration 570 : loss : 0.273248, loss_ce: 0.077275\n",
      "iteration 571 : loss : 0.343789, loss_ce: 0.128785\n",
      "iteration 572 : loss : 0.294000, loss_ce: 0.086638\n",
      "iteration 573 : loss : 0.270867, loss_ce: 0.057782\n",
      "iteration 574 : loss : 0.275499, loss_ce: 0.075945\n",
      "iteration 575 : loss : 0.264972, loss_ce: 0.052015\n",
      "iteration 576 : loss : 0.270369, loss_ce: 0.066027\n",
      "iteration 577 : loss : 0.240497, loss_ce: 0.047991\n",
      "iteration 578 : loss : 0.265790, loss_ce: 0.075747\n",
      "iteration 579 : loss : 0.246170, loss_ce: 0.058072\n",
      "iteration 580 : loss : 0.287192, loss_ce: 0.110612\n",
      "iteration 581 : loss : 0.292323, loss_ce: 0.108809\n",
      "iteration 582 : loss : 0.298611, loss_ce: 0.057383\n",
      "iteration 583 : loss : 0.246490, loss_ce: 0.043762\n",
      "iteration 584 : loss : 0.296078, loss_ce: 0.047001\n",
      "iteration 585 : loss : 0.313472, loss_ce: 0.068543\n",
      "iteration 586 : loss : 0.288214, loss_ce: 0.083004\n",
      "iteration 587 : loss : 0.275754, loss_ce: 0.103162\n",
      "iteration 588 : loss : 0.323694, loss_ce: 0.135221\n",
      "iteration 589 : loss : 0.274970, loss_ce: 0.077985\n",
      "iteration 590 : loss : 0.308896, loss_ce: 0.083382\n",
      "iteration 591 : loss : 0.251620, loss_ce: 0.045847\n",
      "iteration 592 : loss : 0.314678, loss_ce: 0.079775\n",
      "iteration 593 : loss : 0.303475, loss_ce: 0.079994\n",
      "iteration 594 : loss : 0.281930, loss_ce: 0.071340\n",
      "iteration 595 : loss : 0.327713, loss_ce: 0.139296\n",
      "iteration 596 : loss : 0.330653, loss_ce: 0.089421\n",
      "iteration 597 : loss : 0.312931, loss_ce: 0.099001\n",
      "iteration 598 : loss : 0.398721, loss_ce: 0.090981\n",
      "iteration 599 : loss : 0.293911, loss_ce: 0.077078\n",
      "iteration 600 : loss : 0.349593, loss_ce: 0.113991\n",
      "iteration 601 : loss : 0.290821, loss_ce: 0.049472\n",
      "iteration 602 : loss : 0.264797, loss_ce: 0.088195\n",
      "iteration 603 : loss : 0.317838, loss_ce: 0.108344\n",
      "iteration 604 : loss : 0.296832, loss_ce: 0.076527\n",
      "iteration 605 : loss : 0.216258, loss_ce: 0.063453\n",
      "iteration 606 : loss : 0.245176, loss_ce: 0.074096\n",
      "iteration 607 : loss : 0.305919, loss_ce: 0.062011\n",
      "iteration 608 : loss : 0.258730, loss_ce: 0.074666\n",
      "iteration 609 : loss : 0.259180, loss_ce: 0.103092\n",
      "iteration 610 : loss : 0.285549, loss_ce: 0.060955\n",
      "iteration 611 : loss : 0.250700, loss_ce: 0.083335\n",
      "iteration 612 : loss : 0.262832, loss_ce: 0.082458\n",
      "iteration 613 : loss : 0.280290, loss_ce: 0.073887\n",
      "iteration 614 : loss : 0.259200, loss_ce: 0.088865\n",
      "iteration 615 : loss : 0.249216, loss_ce: 0.089022\n",
      "iteration 616 : loss : 0.221669, loss_ce: 0.072016\n",
      "iteration 617 : loss : 0.308115, loss_ce: 0.043148\n",
      "iteration 618 : loss : 0.257528, loss_ce: 0.088319\n",
      "iteration 619 : loss : 0.240280, loss_ce: 0.089381\n",
      "iteration 620 : loss : 0.263060, loss_ce: 0.071468\n",
      "iteration 621 : loss : 0.253812, loss_ce: 0.070796\n",
      "iteration 622 : loss : 0.279674, loss_ce: 0.077956\n",
      "iteration 623 : loss : 0.261117, loss_ce: 0.085567\n",
      "iteration 624 : loss : 0.328081, loss_ce: 0.063642\n",
      "iteration 625 : loss : 0.219246, loss_ce: 0.072090\n",
      "iteration 626 : loss : 0.220831, loss_ce: 0.064430\n",
      "iteration 627 : loss : 0.279322, loss_ce: 0.082283\n",
      "iteration 628 : loss : 0.286072, loss_ce: 0.069803\n",
      "iteration 629 : loss : 0.259248, loss_ce: 0.103728\n",
      "iteration 630 : loss : 0.244358, loss_ce: 0.057384\n",
      "iteration 631 : loss : 0.255265, loss_ce: 0.087135\n",
      "iteration 632 : loss : 0.242267, loss_ce: 0.071726\n",
      "iteration 633 : loss : 0.221659, loss_ce: 0.067903\n",
      "iteration 634 : loss : 0.218082, loss_ce: 0.058113\n",
      "iteration 635 : loss : 0.254758, loss_ce: 0.058815\n",
      "iteration 636 : loss : 0.225550, loss_ce: 0.069465\n",
      "iteration 637 : loss : 0.340572, loss_ce: 0.054724\n",
      "iteration 638 : loss : 0.264756, loss_ce: 0.103324\n",
      "iteration 639 : loss : 0.238721, loss_ce: 0.077096\n",
      "iteration 640 : loss : 0.275154, loss_ce: 0.073131\n",
      "iteration 641 : loss : 0.264954, loss_ce: 0.041040\n",
      "iteration 642 : loss : 0.231558, loss_ce: 0.047834\n",
      "iteration 643 : loss : 0.201113, loss_ce: 0.073277\n",
      "iteration 644 : loss : 0.285529, loss_ce: 0.061798\n",
      "iteration 645 : loss : 0.241070, loss_ce: 0.078056\n",
      "iteration 646 : loss : 0.235025, loss_ce: 0.053775\n",
      "iteration 647 : loss : 0.226763, loss_ce: 0.097540\n",
      "iteration 648 : loss : 0.310397, loss_ce: 0.093114\n",
      "iteration 649 : loss : 0.196882, loss_ce: 0.044825\n",
      "iteration 650 : loss : 0.250186, loss_ce: 0.093038\n",
      "iteration 651 : loss : 0.293218, loss_ce: 0.163548\n",
      " 14%|████▊                             | 7/50 [06:52<43:02, 60.06s/it]iteration 652 : loss : 0.227883, loss_ce: 0.067039\n",
      "iteration 653 : loss : 0.270723, loss_ce: 0.070407\n",
      "iteration 654 : loss : 0.210045, loss_ce: 0.085525\n",
      "iteration 655 : loss : 0.265680, loss_ce: 0.086695\n",
      "iteration 656 : loss : 0.285144, loss_ce: 0.051600\n",
      "iteration 657 : loss : 0.203036, loss_ce: 0.060736\n",
      "iteration 658 : loss : 0.229363, loss_ce: 0.058028\n",
      "iteration 659 : loss : 0.252058, loss_ce: 0.080632\n",
      "iteration 660 : loss : 0.259534, loss_ce: 0.072192\n",
      "iteration 661 : loss : 0.288000, loss_ce: 0.068541\n",
      "iteration 662 : loss : 0.294592, loss_ce: 0.147334\n",
      "iteration 663 : loss : 0.266207, loss_ce: 0.070333\n",
      "iteration 664 : loss : 0.238514, loss_ce: 0.051187\n",
      "iteration 665 : loss : 0.184444, loss_ce: 0.065147\n",
      "iteration 666 : loss : 0.211998, loss_ce: 0.074303\n",
      "iteration 667 : loss : 0.176706, loss_ce: 0.057405\n",
      "iteration 668 : loss : 0.233311, loss_ce: 0.069450\n",
      "iteration 669 : loss : 0.251548, loss_ce: 0.044310\n",
      "iteration 670 : loss : 0.237349, loss_ce: 0.090620\n",
      "iteration 671 : loss : 0.224070, loss_ce: 0.096888\n",
      "iteration 672 : loss : 0.213653, loss_ce: 0.034956\n",
      "iteration 673 : loss : 0.212423, loss_ce: 0.042769\n",
      "iteration 674 : loss : 0.247988, loss_ce: 0.065249\n",
      "iteration 675 : loss : 0.241481, loss_ce: 0.069288\n",
      "iteration 676 : loss : 0.241334, loss_ce: 0.042826\n",
      "iteration 677 : loss : 0.340765, loss_ce: 0.069619\n",
      "iteration 678 : loss : 0.221884, loss_ce: 0.101008\n",
      "iteration 679 : loss : 0.254053, loss_ce: 0.083258\n",
      "iteration 680 : loss : 0.252342, loss_ce: 0.057072\n",
      "iteration 681 : loss : 0.195498, loss_ce: 0.056459\n",
      "iteration 682 : loss : 0.364157, loss_ce: 0.031893\n",
      "iteration 683 : loss : 0.212215, loss_ce: 0.070661\n",
      "iteration 684 : loss : 0.240537, loss_ce: 0.035735\n",
      "iteration 685 : loss : 0.227990, loss_ce: 0.091111\n",
      "iteration 686 : loss : 0.222899, loss_ce: 0.113183\n",
      "iteration 687 : loss : 0.256335, loss_ce: 0.058997\n",
      "iteration 688 : loss : 0.214280, loss_ce: 0.069456\n",
      "iteration 689 : loss : 0.235224, loss_ce: 0.045284\n",
      "iteration 690 : loss : 0.281562, loss_ce: 0.072741\n",
      "iteration 691 : loss : 0.188294, loss_ce: 0.059483\n",
      "iteration 692 : loss : 0.197866, loss_ce: 0.047941\n",
      "iteration 693 : loss : 0.193111, loss_ce: 0.071843\n",
      "iteration 694 : loss : 0.210944, loss_ce: 0.061247\n",
      "iteration 695 : loss : 0.238783, loss_ce: 0.099547\n",
      "iteration 696 : loss : 0.285854, loss_ce: 0.042238\n",
      "iteration 697 : loss : 0.273837, loss_ce: 0.031161\n",
      "iteration 698 : loss : 0.196516, loss_ce: 0.023914\n",
      "iteration 699 : loss : 0.260777, loss_ce: 0.101345\n",
      "iteration 700 : loss : 0.246509, loss_ce: 0.071940\n",
      "iteration 701 : loss : 0.268291, loss_ce: 0.068460\n",
      "iteration 702 : loss : 0.288142, loss_ce: 0.052892\n",
      "iteration 703 : loss : 0.272660, loss_ce: 0.088368\n",
      "iteration 704 : loss : 0.252004, loss_ce: 0.067163\n",
      "iteration 705 : loss : 0.254856, loss_ce: 0.100477\n",
      "iteration 706 : loss : 0.221262, loss_ce: 0.057366\n",
      "iteration 707 : loss : 0.234385, loss_ce: 0.048919\n",
      "iteration 708 : loss : 0.271101, loss_ce: 0.036665\n",
      "iteration 709 : loss : 0.300271, loss_ce: 0.126301\n",
      "iteration 710 : loss : 0.281182, loss_ce: 0.106889\n",
      "iteration 711 : loss : 0.255767, loss_ce: 0.055318\n",
      "iteration 712 : loss : 0.261172, loss_ce: 0.080376\n",
      "iteration 713 : loss : 0.314954, loss_ce: 0.046333\n",
      "iteration 714 : loss : 0.207293, loss_ce: 0.049173\n",
      "iteration 715 : loss : 0.220489, loss_ce: 0.051385\n",
      "iteration 716 : loss : 0.242449, loss_ce: 0.062708\n",
      "iteration 717 : loss : 0.252589, loss_ce: 0.074502\n",
      "iteration 718 : loss : 0.291453, loss_ce: 0.087962\n",
      "iteration 719 : loss : 0.269702, loss_ce: 0.062801\n",
      "iteration 720 : loss : 0.304281, loss_ce: 0.083362\n",
      "iteration 721 : loss : 0.219418, loss_ce: 0.056088\n",
      "iteration 722 : loss : 0.227600, loss_ce: 0.067135\n",
      "iteration 723 : loss : 0.179775, loss_ce: 0.062596\n",
      "iteration 724 : loss : 0.182816, loss_ce: 0.063792\n",
      "iteration 725 : loss : 0.202943, loss_ce: 0.050867\n",
      "iteration 726 : loss : 0.211494, loss_ce: 0.066751\n",
      "iteration 727 : loss : 0.262750, loss_ce: 0.085132\n",
      "iteration 728 : loss : 0.214859, loss_ce: 0.097691\n",
      "iteration 729 : loss : 0.232944, loss_ce: 0.099267\n",
      "iteration 730 : loss : 0.233849, loss_ce: 0.083981\n",
      "iteration 731 : loss : 0.224390, loss_ce: 0.043625\n",
      "iteration 732 : loss : 0.260312, loss_ce: 0.065654\n",
      "iteration 733 : loss : 0.229236, loss_ce: 0.082910\n",
      "iteration 734 : loss : 0.217560, loss_ce: 0.042618\n",
      "iteration 735 : loss : 0.218827, loss_ce: 0.069452\n",
      "iteration 736 : loss : 0.164862, loss_ce: 0.029761\n",
      "iteration 737 : loss : 0.191425, loss_ce: 0.062080\n",
      "iteration 738 : loss : 0.272313, loss_ce: 0.051819\n",
      "iteration 739 : loss : 0.208819, loss_ce: 0.071639\n",
      "iteration 740 : loss : 0.196726, loss_ce: 0.046793\n",
      "iteration 741 : loss : 0.227851, loss_ce: 0.040351\n",
      "iteration 742 : loss : 0.210053, loss_ce: 0.086841\n",
      "iteration 743 : loss : 0.249083, loss_ce: 0.052051\n",
      "iteration 744 : loss : 0.449248, loss_ce: 0.001096\n",
      " 16%|█████▍                            | 8/50 [07:50<41:38, 59.49s/it]iteration 745 : loss : 0.242959, loss_ce: 0.079988\n",
      "iteration 746 : loss : 0.224864, loss_ce: 0.035452\n",
      "iteration 747 : loss : 0.231148, loss_ce: 0.096143\n",
      "iteration 748 : loss : 0.297742, loss_ce: 0.138125\n",
      "iteration 749 : loss : 0.260335, loss_ce: 0.082576\n",
      "iteration 750 : loss : 0.196111, loss_ce: 0.056538\n",
      "iteration 751 : loss : 0.308622, loss_ce: 0.068169\n",
      "iteration 752 : loss : 0.241969, loss_ce: 0.069593\n",
      "iteration 753 : loss : 0.239915, loss_ce: 0.078531\n",
      "iteration 754 : loss : 0.214368, loss_ce: 0.096133\n",
      "iteration 755 : loss : 0.208025, loss_ce: 0.048494\n",
      "iteration 756 : loss : 0.229919, loss_ce: 0.054586\n",
      "iteration 757 : loss : 0.253346, loss_ce: 0.059477\n",
      "iteration 758 : loss : 0.207594, loss_ce: 0.081955\n",
      "iteration 759 : loss : 0.225670, loss_ce: 0.068782\n",
      "iteration 760 : loss : 0.178752, loss_ce: 0.059126\n",
      "iteration 761 : loss : 0.193774, loss_ce: 0.055155\n",
      "iteration 762 : loss : 0.243749, loss_ce: 0.058348\n",
      "iteration 763 : loss : 0.205360, loss_ce: 0.056677\n",
      "iteration 764 : loss : 0.233866, loss_ce: 0.063009\n",
      "iteration 765 : loss : 0.171394, loss_ce: 0.060469\n",
      "iteration 766 : loss : 0.205745, loss_ce: 0.104821\n",
      "iteration 767 : loss : 0.281794, loss_ce: 0.055572\n",
      "iteration 768 : loss : 0.231365, loss_ce: 0.086604\n",
      "iteration 769 : loss : 0.170257, loss_ce: 0.058691\n",
      "iteration 770 : loss : 0.217829, loss_ce: 0.062541\n",
      "iteration 771 : loss : 0.237186, loss_ce: 0.059057\n",
      "iteration 772 : loss : 0.213467, loss_ce: 0.054813\n",
      "iteration 773 : loss : 0.252771, loss_ce: 0.053841\n",
      "iteration 774 : loss : 0.163975, loss_ce: 0.041460\n",
      "iteration 775 : loss : 0.238451, loss_ce: 0.064881\n",
      "iteration 776 : loss : 0.165259, loss_ce: 0.055419\n",
      "iteration 777 : loss : 0.164061, loss_ce: 0.059605\n",
      "iteration 778 : loss : 0.214573, loss_ce: 0.058860\n",
      "iteration 779 : loss : 0.221729, loss_ce: 0.058091\n",
      "iteration 780 : loss : 0.223802, loss_ce: 0.064541\n",
      "iteration 781 : loss : 0.176441, loss_ce: 0.069434\n",
      "iteration 782 : loss : 0.158211, loss_ce: 0.039797\n",
      "iteration 783 : loss : 0.198838, loss_ce: 0.055502\n",
      "iteration 784 : loss : 0.185697, loss_ce: 0.033969\n",
      "iteration 785 : loss : 0.186395, loss_ce: 0.053214\n",
      "iteration 786 : loss : 0.227459, loss_ce: 0.087565\n",
      "iteration 787 : loss : 0.287106, loss_ce: 0.084177\n",
      "iteration 788 : loss : 0.230250, loss_ce: 0.056972\n",
      "iteration 789 : loss : 0.184513, loss_ce: 0.062052\n",
      "iteration 790 : loss : 0.178574, loss_ce: 0.071400\n",
      "iteration 791 : loss : 0.200142, loss_ce: 0.040729\n",
      "iteration 792 : loss : 0.169461, loss_ce: 0.040653\n",
      "iteration 793 : loss : 0.211548, loss_ce: 0.048837\n",
      "iteration 794 : loss : 0.191504, loss_ce: 0.057754\n",
      "iteration 795 : loss : 0.229663, loss_ce: 0.061010\n",
      "iteration 796 : loss : 0.223923, loss_ce: 0.043540\n",
      "iteration 797 : loss : 0.141677, loss_ce: 0.050296\n",
      "iteration 798 : loss : 0.240209, loss_ce: 0.051912\n",
      "iteration 799 : loss : 0.206719, loss_ce: 0.065357\n",
      "iteration 800 : loss : 0.217488, loss_ce: 0.100855\n",
      "iteration 801 : loss : 0.207876, loss_ce: 0.072517\n",
      "iteration 802 : loss : 0.181240, loss_ce: 0.052819\n",
      "iteration 803 : loss : 0.188839, loss_ce: 0.058660\n",
      "iteration 804 : loss : 0.188334, loss_ce: 0.062314\n",
      "iteration 805 : loss : 0.199139, loss_ce: 0.061826\n",
      "iteration 806 : loss : 0.217558, loss_ce: 0.081377\n",
      "iteration 807 : loss : 0.203252, loss_ce: 0.048692\n",
      "iteration 808 : loss : 0.226839, loss_ce: 0.062620\n",
      "iteration 809 : loss : 0.264719, loss_ce: 0.032866\n",
      "iteration 810 : loss : 0.206336, loss_ce: 0.055599\n",
      "iteration 811 : loss : 0.237659, loss_ce: 0.023861\n",
      "iteration 812 : loss : 0.202779, loss_ce: 0.046008\n",
      "iteration 813 : loss : 0.181885, loss_ce: 0.066399\n",
      "iteration 814 : loss : 0.265010, loss_ce: 0.067773\n",
      "iteration 815 : loss : 0.212937, loss_ce: 0.071595\n",
      "iteration 816 : loss : 0.232817, loss_ce: 0.042394\n",
      "iteration 817 : loss : 0.184439, loss_ce: 0.055907\n",
      "iteration 818 : loss : 0.185508, loss_ce: 0.049074\n",
      "iteration 819 : loss : 0.196244, loss_ce: 0.066716\n",
      "iteration 820 : loss : 0.183198, loss_ce: 0.067169\n",
      "iteration 821 : loss : 0.153162, loss_ce: 0.030219\n",
      "iteration 822 : loss : 0.253241, loss_ce: 0.030224\n",
      "iteration 823 : loss : 0.144263, loss_ce: 0.032139\n",
      "iteration 824 : loss : 0.168059, loss_ce: 0.028408\n",
      "iteration 825 : loss : 0.181273, loss_ce: 0.058655\n",
      "iteration 826 : loss : 0.183282, loss_ce: 0.060585\n",
      "iteration 827 : loss : 0.211489, loss_ce: 0.070895\n",
      "iteration 828 : loss : 0.161313, loss_ce: 0.048683\n",
      "iteration 829 : loss : 0.172280, loss_ce: 0.057184\n",
      "iteration 830 : loss : 0.280796, loss_ce: 0.037941\n",
      "iteration 831 : loss : 0.180451, loss_ce: 0.057664\n",
      "iteration 832 : loss : 0.197826, loss_ce: 0.048574\n",
      "iteration 833 : loss : 0.154665, loss_ce: 0.046010\n",
      "iteration 834 : loss : 0.223348, loss_ce: 0.056102\n",
      "iteration 835 : loss : 0.197839, loss_ce: 0.052581\n",
      "iteration 836 : loss : 0.217253, loss_ce: 0.042920\n",
      "iteration 837 : loss : 0.315808, loss_ce: 0.029679\n",
      " 18%|██████                            | 9/50 [08:47<40:11, 58.81s/it]iteration 838 : loss : 0.199120, loss_ce: 0.020597\n",
      "iteration 839 : loss : 0.223328, loss_ce: 0.061010\n",
      "iteration 840 : loss : 0.174811, loss_ce: 0.045273\n",
      "iteration 841 : loss : 0.185896, loss_ce: 0.072802\n",
      "iteration 842 : loss : 0.257420, loss_ce: 0.043527\n",
      "iteration 843 : loss : 0.288497, loss_ce: 0.098757\n",
      "iteration 844 : loss : 0.213821, loss_ce: 0.080174\n",
      "iteration 845 : loss : 0.169141, loss_ce: 0.047948\n",
      "iteration 846 : loss : 0.258974, loss_ce: 0.057405\n",
      "iteration 847 : loss : 0.188446, loss_ce: 0.054292\n",
      "iteration 848 : loss : 0.203259, loss_ce: 0.070829\n",
      "iteration 849 : loss : 0.216952, loss_ce: 0.065683\n",
      "iteration 850 : loss : 0.169729, loss_ce: 0.057170\n",
      "iteration 851 : loss : 0.263367, loss_ce: 0.053338\n",
      "iteration 852 : loss : 0.181654, loss_ce: 0.060858\n",
      "iteration 853 : loss : 0.162696, loss_ce: 0.054613\n",
      "iteration 854 : loss : 0.264532, loss_ce: 0.050538\n",
      "iteration 855 : loss : 0.204338, loss_ce: 0.050511\n",
      "iteration 856 : loss : 0.217207, loss_ce: 0.079451\n",
      "iteration 857 : loss : 0.246249, loss_ce: 0.020355\n",
      "iteration 858 : loss : 0.242072, loss_ce: 0.059714\n",
      "iteration 859 : loss : 0.182349, loss_ce: 0.025153\n",
      "iteration 860 : loss : 0.192399, loss_ce: 0.084009\n",
      "iteration 861 : loss : 0.197971, loss_ce: 0.054290\n",
      "iteration 862 : loss : 0.268575, loss_ce: 0.074626\n",
      "iteration 863 : loss : 0.200523, loss_ce: 0.072342\n",
      "iteration 864 : loss : 0.234546, loss_ce: 0.044720\n",
      "iteration 865 : loss : 0.238647, loss_ce: 0.061349\n",
      "iteration 866 : loss : 0.212187, loss_ce: 0.066725\n",
      "iteration 867 : loss : 0.186590, loss_ce: 0.058327\n",
      "iteration 868 : loss : 0.179378, loss_ce: 0.049685\n",
      "iteration 869 : loss : 0.205763, loss_ce: 0.037078\n",
      "iteration 870 : loss : 0.213059, loss_ce: 0.051456\n",
      "iteration 871 : loss : 0.213426, loss_ce: 0.038491\n",
      "iteration 872 : loss : 0.159182, loss_ce: 0.063324\n",
      "iteration 873 : loss : 0.252929, loss_ce: 0.057958\n",
      "iteration 874 : loss : 0.187574, loss_ce: 0.054930\n",
      "iteration 875 : loss : 0.159742, loss_ce: 0.051489\n",
      "iteration 876 : loss : 0.170098, loss_ce: 0.035273\n",
      "iteration 877 : loss : 0.135963, loss_ce: 0.044505\n",
      "iteration 878 : loss : 0.229434, loss_ce: 0.050743\n",
      "iteration 879 : loss : 0.204303, loss_ce: 0.050246\n",
      "iteration 880 : loss : 0.164799, loss_ce: 0.057385\n",
      "iteration 881 : loss : 0.200107, loss_ce: 0.036931\n",
      "iteration 882 : loss : 0.195397, loss_ce: 0.043369\n",
      "iteration 883 : loss : 0.191296, loss_ce: 0.071148\n",
      "iteration 884 : loss : 0.143057, loss_ce: 0.057843\n",
      "iteration 885 : loss : 0.155992, loss_ce: 0.051883\n",
      "iteration 886 : loss : 0.204807, loss_ce: 0.040838\n",
      "iteration 887 : loss : 0.150954, loss_ce: 0.061389\n",
      "iteration 888 : loss : 0.152757, loss_ce: 0.051007\n",
      "iteration 889 : loss : 0.203354, loss_ce: 0.051130\n",
      "iteration 890 : loss : 0.177938, loss_ce: 0.030672\n",
      "iteration 891 : loss : 0.174497, loss_ce: 0.044652\n",
      "iteration 892 : loss : 0.199398, loss_ce: 0.043255\n",
      "iteration 893 : loss : 0.190792, loss_ce: 0.031553\n",
      "iteration 894 : loss : 0.177545, loss_ce: 0.041142\n",
      "iteration 895 : loss : 0.167394, loss_ce: 0.043625\n",
      "iteration 896 : loss : 0.198195, loss_ce: 0.043441\n",
      "iteration 897 : loss : 0.143326, loss_ce: 0.031503\n",
      "iteration 898 : loss : 0.207317, loss_ce: 0.042028\n",
      "iteration 899 : loss : 0.245985, loss_ce: 0.023551\n",
      "iteration 900 : loss : 0.175497, loss_ce: 0.072482\n",
      "iteration 901 : loss : 0.170029, loss_ce: 0.052111\n",
      "iteration 902 : loss : 0.207898, loss_ce: 0.017093\n",
      "iteration 903 : loss : 0.184027, loss_ce: 0.055375\n",
      "iteration 904 : loss : 0.166307, loss_ce: 0.058374\n",
      "iteration 905 : loss : 0.151165, loss_ce: 0.045398\n",
      "iteration 906 : loss : 0.181533, loss_ce: 0.037316\n",
      "iteration 907 : loss : 0.167546, loss_ce: 0.056283\n",
      "iteration 908 : loss : 0.164119, loss_ce: 0.077371\n",
      "iteration 909 : loss : 0.189417, loss_ce: 0.078146\n",
      "iteration 910 : loss : 0.180387, loss_ce: 0.035823\n",
      "iteration 911 : loss : 0.205613, loss_ce: 0.076350\n",
      "iteration 912 : loss : 0.146369, loss_ce: 0.036704\n",
      "iteration 913 : loss : 0.195617, loss_ce: 0.077574\n",
      "iteration 914 : loss : 0.161360, loss_ce: 0.053177\n",
      "iteration 915 : loss : 0.126013, loss_ce: 0.038091\n",
      "iteration 916 : loss : 0.204850, loss_ce: 0.056313\n",
      "iteration 917 : loss : 0.178158, loss_ce: 0.039250\n",
      "iteration 918 : loss : 0.162663, loss_ce: 0.053809\n",
      "iteration 919 : loss : 0.162374, loss_ce: 0.033490\n",
      "iteration 920 : loss : 0.218973, loss_ce: 0.057800\n",
      "iteration 921 : loss : 0.159871, loss_ce: 0.050884\n",
      "iteration 922 : loss : 0.148872, loss_ce: 0.065807\n",
      "iteration 923 : loss : 0.168976, loss_ce: 0.057037\n",
      "iteration 924 : loss : 0.154387, loss_ce: 0.039628\n",
      "iteration 925 : loss : 0.150553, loss_ce: 0.061871\n",
      "iteration 926 : loss : 0.146215, loss_ce: 0.039606\n",
      "iteration 927 : loss : 0.192940, loss_ce: 0.047011\n",
      "iteration 928 : loss : 0.141456, loss_ce: 0.038599\n",
      "iteration 929 : loss : 0.175534, loss_ce: 0.048873\n",
      "iteration 930 : loss : 0.348570, loss_ce: 0.068443\n",
      " 20%|██████▌                          | 10/50 [09:45<38:56, 58.42s/it]iteration 931 : loss : 0.175983, loss_ce: 0.041514\n",
      "iteration 932 : loss : 0.148085, loss_ce: 0.043849\n",
      "iteration 933 : loss : 0.159656, loss_ce: 0.047311\n",
      "iteration 934 : loss : 0.175615, loss_ce: 0.056995\n",
      "iteration 935 : loss : 0.205794, loss_ce: 0.076318\n",
      "iteration 936 : loss : 0.133158, loss_ce: 0.037181\n",
      "iteration 937 : loss : 0.174604, loss_ce: 0.045518\n",
      "iteration 938 : loss : 0.127977, loss_ce: 0.031288\n",
      "iteration 939 : loss : 0.155165, loss_ce: 0.037376\n",
      "iteration 940 : loss : 0.174147, loss_ce: 0.051750\n",
      "iteration 941 : loss : 0.134697, loss_ce: 0.058962\n",
      "iteration 942 : loss : 0.170797, loss_ce: 0.031092\n",
      "iteration 943 : loss : 0.178848, loss_ce: 0.039637\n",
      "iteration 944 : loss : 0.203583, loss_ce: 0.054723\n",
      "iteration 945 : loss : 0.196835, loss_ce: 0.043876\n",
      "iteration 946 : loss : 0.224147, loss_ce: 0.048278\n",
      "iteration 947 : loss : 0.143343, loss_ce: 0.041456\n",
      "iteration 948 : loss : 0.185112, loss_ce: 0.034147\n",
      "iteration 949 : loss : 0.170199, loss_ce: 0.044826\n",
      "iteration 950 : loss : 0.147688, loss_ce: 0.046410\n",
      "iteration 951 : loss : 0.181239, loss_ce: 0.036163\n",
      "iteration 952 : loss : 0.143067, loss_ce: 0.036043\n",
      "iteration 953 : loss : 0.236786, loss_ce: 0.037632\n",
      "iteration 954 : loss : 0.143846, loss_ce: 0.056145\n",
      "iteration 955 : loss : 0.154714, loss_ce: 0.019140\n",
      "iteration 956 : loss : 0.134503, loss_ce: 0.035918\n",
      "iteration 957 : loss : 0.240556, loss_ce: 0.056277\n",
      "iteration 958 : loss : 0.160892, loss_ce: 0.061527\n",
      "iteration 959 : loss : 0.179618, loss_ce: 0.046871\n",
      "iteration 960 : loss : 0.188536, loss_ce: 0.040203\n",
      "iteration 961 : loss : 0.162848, loss_ce: 0.042134\n",
      "iteration 962 : loss : 0.152033, loss_ce: 0.042233\n",
      "iteration 963 : loss : 0.172940, loss_ce: 0.052461\n",
      "iteration 964 : loss : 0.208068, loss_ce: 0.057905\n",
      "iteration 965 : loss : 0.190551, loss_ce: 0.063249\n",
      "iteration 966 : loss : 0.231801, loss_ce: 0.048180\n",
      "iteration 967 : loss : 0.213963, loss_ce: 0.053065\n",
      "iteration 968 : loss : 0.180827, loss_ce: 0.086925\n",
      "iteration 969 : loss : 0.252679, loss_ce: 0.070910\n",
      "iteration 970 : loss : 0.186061, loss_ce: 0.041874\n",
      "iteration 971 : loss : 0.187852, loss_ce: 0.036635\n",
      "iteration 972 : loss : 0.199826, loss_ce: 0.033339\n",
      "iteration 973 : loss : 0.200364, loss_ce: 0.063069\n",
      "iteration 974 : loss : 0.201670, loss_ce: 0.061998\n",
      "iteration 975 : loss : 0.157960, loss_ce: 0.052303\n",
      "iteration 976 : loss : 0.179805, loss_ce: 0.041610\n",
      "iteration 977 : loss : 0.208517, loss_ce: 0.054933\n",
      "iteration 978 : loss : 0.214760, loss_ce: 0.052513\n",
      "iteration 979 : loss : 0.199001, loss_ce: 0.038794\n",
      "iteration 980 : loss : 0.169163, loss_ce: 0.049588\n",
      "iteration 981 : loss : 0.188446, loss_ce: 0.036913\n",
      "iteration 982 : loss : 0.153497, loss_ce: 0.055970\n",
      "iteration 983 : loss : 0.170813, loss_ce: 0.055707\n",
      "iteration 984 : loss : 0.128024, loss_ce: 0.030960\n",
      "iteration 985 : loss : 0.165012, loss_ce: 0.065547\n",
      "iteration 986 : loss : 0.159119, loss_ce: 0.053662\n",
      "iteration 987 : loss : 0.159462, loss_ce: 0.052570\n",
      "iteration 988 : loss : 0.161992, loss_ce: 0.069510\n",
      "iteration 989 : loss : 0.209653, loss_ce: 0.054802\n",
      "iteration 990 : loss : 0.183699, loss_ce: 0.063934\n",
      "iteration 991 : loss : 0.154946, loss_ce: 0.037619\n",
      "iteration 992 : loss : 0.135792, loss_ce: 0.075984\n",
      "iteration 993 : loss : 0.210199, loss_ce: 0.023381\n",
      "iteration 994 : loss : 0.180637, loss_ce: 0.077799\n",
      "iteration 995 : loss : 0.131263, loss_ce: 0.032403\n",
      "iteration 996 : loss : 0.181126, loss_ce: 0.053248\n",
      "iteration 997 : loss : 0.205514, loss_ce: 0.040988\n",
      "iteration 998 : loss : 0.215688, loss_ce: 0.070050\n",
      "iteration 999 : loss : 0.171246, loss_ce: 0.029913\n",
      "iteration 1000 : loss : 0.119289, loss_ce: 0.035508\n",
      "iteration 1001 : loss : 0.135256, loss_ce: 0.035979\n",
      "iteration 1002 : loss : 0.143985, loss_ce: 0.058733\n",
      "iteration 1003 : loss : 0.198903, loss_ce: 0.052740\n",
      "iteration 1004 : loss : 0.133687, loss_ce: 0.028592\n",
      "iteration 1005 : loss : 0.199050, loss_ce: 0.043956\n",
      "iteration 1006 : loss : 0.136217, loss_ce: 0.053225\n",
      "iteration 1007 : loss : 0.149592, loss_ce: 0.048336\n",
      "iteration 1008 : loss : 0.213594, loss_ce: 0.038438\n",
      "iteration 1009 : loss : 0.111422, loss_ce: 0.033218\n",
      "iteration 1010 : loss : 0.211460, loss_ce: 0.048344\n",
      "iteration 1011 : loss : 0.163780, loss_ce: 0.072583\n",
      "iteration 1012 : loss : 0.178153, loss_ce: 0.050120\n",
      "iteration 1013 : loss : 0.215693, loss_ce: 0.055773\n",
      "iteration 1014 : loss : 0.157653, loss_ce: 0.039277\n",
      "iteration 1015 : loss : 0.188695, loss_ce: 0.046975\n",
      "iteration 1016 : loss : 0.114712, loss_ce: 0.033933\n",
      "iteration 1017 : loss : 0.168643, loss_ce: 0.047377\n",
      "iteration 1018 : loss : 0.137597, loss_ce: 0.039934\n",
      "iteration 1019 : loss : 0.161844, loss_ce: 0.038918\n",
      "iteration 1020 : loss : 0.212341, loss_ce: 0.040131\n",
      "iteration 1021 : loss : 0.114737, loss_ce: 0.043140\n",
      "iteration 1022 : loss : 0.161038, loss_ce: 0.041521\n",
      "iteration 1023 : loss : 0.297281, loss_ce: 0.081655\n",
      " 22%|███████▎                         | 11/50 [10:43<37:54, 58.32s/it]iteration 1024 : loss : 0.133223, loss_ce: 0.035804\n",
      "iteration 1025 : loss : 0.174431, loss_ce: 0.048000\n",
      "iteration 1026 : loss : 0.158313, loss_ce: 0.033363\n",
      "iteration 1027 : loss : 0.156993, loss_ce: 0.028163\n",
      "iteration 1028 : loss : 0.182678, loss_ce: 0.052899\n",
      "iteration 1029 : loss : 0.189282, loss_ce: 0.063143\n",
      "iteration 1030 : loss : 0.132726, loss_ce: 0.050069\n",
      "iteration 1031 : loss : 0.146954, loss_ce: 0.037034\n",
      "iteration 1032 : loss : 0.166866, loss_ce: 0.070709\n",
      "iteration 1033 : loss : 0.180130, loss_ce: 0.037721\n",
      "iteration 1034 : loss : 0.157125, loss_ce: 0.060813\n",
      "iteration 1035 : loss : 0.185147, loss_ce: 0.054785\n",
      "iteration 1036 : loss : 0.153623, loss_ce: 0.033655\n",
      "iteration 1037 : loss : 0.118432, loss_ce: 0.043336\n",
      "iteration 1038 : loss : 0.175110, loss_ce: 0.055199\n",
      "iteration 1039 : loss : 0.130391, loss_ce: 0.033388\n",
      "iteration 1040 : loss : 0.133143, loss_ce: 0.048237\n",
      "iteration 1041 : loss : 0.134063, loss_ce: 0.045876\n",
      "iteration 1042 : loss : 0.128254, loss_ce: 0.031281\n",
      "iteration 1043 : loss : 0.138247, loss_ce: 0.040615\n",
      "iteration 1044 : loss : 0.190678, loss_ce: 0.034216\n",
      "iteration 1045 : loss : 0.162201, loss_ce: 0.027225\n",
      "iteration 1046 : loss : 0.168887, loss_ce: 0.024455\n",
      "iteration 1047 : loss : 0.140104, loss_ce: 0.045068\n",
      "iteration 1048 : loss : 0.216472, loss_ce: 0.052428\n",
      "iteration 1049 : loss : 0.122842, loss_ce: 0.048103\n",
      "iteration 1050 : loss : 0.169909, loss_ce: 0.052340\n",
      "iteration 1051 : loss : 0.192448, loss_ce: 0.038801\n",
      "iteration 1052 : loss : 0.119624, loss_ce: 0.035755\n",
      "iteration 1053 : loss : 0.183901, loss_ce: 0.047234\n",
      "iteration 1054 : loss : 0.104205, loss_ce: 0.027510\n",
      "iteration 1055 : loss : 0.099048, loss_ce: 0.020558\n",
      "iteration 1056 : loss : 0.119484, loss_ce: 0.032654\n",
      "iteration 1057 : loss : 0.160222, loss_ce: 0.038742\n",
      "iteration 1058 : loss : 0.135108, loss_ce: 0.040404\n",
      "iteration 1059 : loss : 0.128620, loss_ce: 0.038995\n",
      "iteration 1060 : loss : 0.141461, loss_ce: 0.035084\n",
      "iteration 1061 : loss : 0.171033, loss_ce: 0.047894\n",
      "iteration 1062 : loss : 0.150863, loss_ce: 0.053600\n",
      "iteration 1063 : loss : 0.119585, loss_ce: 0.035384\n",
      "iteration 1064 : loss : 0.123693, loss_ce: 0.023097\n",
      "iteration 1065 : loss : 0.142480, loss_ce: 0.043455\n",
      "iteration 1066 : loss : 0.149320, loss_ce: 0.069084\n",
      "iteration 1067 : loss : 0.122540, loss_ce: 0.034298\n",
      "iteration 1068 : loss : 0.142169, loss_ce: 0.038101\n",
      "iteration 1069 : loss : 0.180215, loss_ce: 0.062655\n",
      "iteration 1070 : loss : 0.143009, loss_ce: 0.021965\n",
      "iteration 1071 : loss : 0.136262, loss_ce: 0.026022\n",
      "iteration 1072 : loss : 0.174885, loss_ce: 0.042542\n",
      "iteration 1073 : loss : 0.153960, loss_ce: 0.045937\n",
      "iteration 1074 : loss : 0.145638, loss_ce: 0.048409\n",
      "iteration 1075 : loss : 0.138150, loss_ce: 0.061348\n",
      "iteration 1076 : loss : 0.166660, loss_ce: 0.056430\n",
      "iteration 1077 : loss : 0.152351, loss_ce: 0.055307\n",
      "iteration 1078 : loss : 0.118696, loss_ce: 0.028691\n",
      "iteration 1079 : loss : 0.145323, loss_ce: 0.017408\n",
      "iteration 1080 : loss : 0.136633, loss_ce: 0.031259\n",
      "iteration 1081 : loss : 0.180000, loss_ce: 0.064055\n",
      "iteration 1082 : loss : 0.171297, loss_ce: 0.042371\n",
      "iteration 1083 : loss : 0.134495, loss_ce: 0.060854\n",
      "iteration 1084 : loss : 0.152299, loss_ce: 0.058371\n",
      "iteration 1085 : loss : 0.132094, loss_ce: 0.026882\n",
      "iteration 1086 : loss : 0.176314, loss_ce: 0.048373\n",
      "iteration 1087 : loss : 0.145893, loss_ce: 0.033993\n",
      "iteration 1088 : loss : 0.127782, loss_ce: 0.020009\n",
      "iteration 1089 : loss : 0.155135, loss_ce: 0.052330\n",
      "iteration 1090 : loss : 0.163394, loss_ce: 0.042913\n",
      "iteration 1091 : loss : 0.139183, loss_ce: 0.045418\n",
      "iteration 1092 : loss : 0.165600, loss_ce: 0.020582\n",
      "iteration 1093 : loss : 0.155377, loss_ce: 0.053826\n",
      "iteration 1094 : loss : 0.138997, loss_ce: 0.042615\n",
      "iteration 1095 : loss : 0.183735, loss_ce: 0.056467\n",
      "iteration 1096 : loss : 0.159564, loss_ce: 0.086611\n",
      "iteration 1097 : loss : 0.224255, loss_ce: 0.029761\n",
      "iteration 1098 : loss : 0.169418, loss_ce: 0.030500\n",
      "iteration 1099 : loss : 0.160894, loss_ce: 0.042478\n",
      "iteration 1100 : loss : 0.147685, loss_ce: 0.056442\n",
      "iteration 1101 : loss : 0.121100, loss_ce: 0.036645\n",
      "iteration 1102 : loss : 0.181338, loss_ce: 0.035427\n",
      "iteration 1103 : loss : 0.135510, loss_ce: 0.054441\n",
      "iteration 1104 : loss : 0.129492, loss_ce: 0.041490\n",
      "iteration 1105 : loss : 0.148726, loss_ce: 0.023011\n",
      "iteration 1106 : loss : 0.177942, loss_ce: 0.061247\n",
      "iteration 1107 : loss : 0.147037, loss_ce: 0.040475\n",
      "iteration 1108 : loss : 0.128685, loss_ce: 0.033617\n",
      "iteration 1109 : loss : 0.113567, loss_ce: 0.044496\n",
      "iteration 1110 : loss : 0.184241, loss_ce: 0.050304\n",
      "iteration 1111 : loss : 0.174912, loss_ce: 0.039169\n",
      "iteration 1112 : loss : 0.169696, loss_ce: 0.043959\n",
      "iteration 1113 : loss : 0.218037, loss_ce: 0.047773\n",
      "iteration 1114 : loss : 0.167316, loss_ce: 0.028878\n",
      "iteration 1115 : loss : 0.190873, loss_ce: 0.035509\n",
      "iteration 1116 : loss : 0.307992, loss_ce: 0.050495\n",
      " 24%|███████▉                         | 12/50 [11:42<37:03, 58.52s/it]iteration 1117 : loss : 0.193708, loss_ce: 0.031332\n",
      "iteration 1118 : loss : 0.160899, loss_ce: 0.042380\n",
      "iteration 1119 : loss : 0.212082, loss_ce: 0.055786\n",
      "iteration 1120 : loss : 0.160735, loss_ce: 0.044390\n",
      "iteration 1121 : loss : 0.172858, loss_ce: 0.050372\n",
      "iteration 1122 : loss : 0.145760, loss_ce: 0.048116\n",
      "iteration 1123 : loss : 0.155371, loss_ce: 0.068213\n",
      "iteration 1124 : loss : 0.123577, loss_ce: 0.028421\n",
      "iteration 1125 : loss : 0.150516, loss_ce: 0.037457\n",
      "iteration 1126 : loss : 0.187444, loss_ce: 0.029156\n",
      "iteration 1127 : loss : 0.155407, loss_ce: 0.049565\n",
      "iteration 1128 : loss : 0.226531, loss_ce: 0.013846\n",
      "iteration 1129 : loss : 0.126295, loss_ce: 0.038693\n",
      "iteration 1130 : loss : 0.134208, loss_ce: 0.056372\n",
      "iteration 1131 : loss : 0.176145, loss_ce: 0.033662\n",
      "iteration 1132 : loss : 0.136715, loss_ce: 0.039945\n",
      "iteration 1133 : loss : 0.134649, loss_ce: 0.041248\n",
      "iteration 1134 : loss : 0.152623, loss_ce: 0.047672\n",
      "iteration 1135 : loss : 0.176295, loss_ce: 0.056410\n",
      "iteration 1136 : loss : 0.157138, loss_ce: 0.040434\n",
      "iteration 1137 : loss : 0.111618, loss_ce: 0.036948\n",
      "iteration 1138 : loss : 0.118489, loss_ce: 0.048912\n",
      "iteration 1139 : loss : 0.179210, loss_ce: 0.071029\n",
      "iteration 1140 : loss : 0.134364, loss_ce: 0.033015\n",
      "iteration 1141 : loss : 0.129125, loss_ce: 0.035082\n",
      "iteration 1142 : loss : 0.119027, loss_ce: 0.038944\n",
      "iteration 1143 : loss : 0.115836, loss_ce: 0.041461\n",
      "iteration 1144 : loss : 0.240263, loss_ce: 0.028377\n",
      "iteration 1145 : loss : 0.161589, loss_ce: 0.036652\n",
      "iteration 1146 : loss : 0.181801, loss_ce: 0.030795\n",
      "iteration 1147 : loss : 0.157920, loss_ce: 0.023354\n",
      "iteration 1148 : loss : 0.240152, loss_ce: 0.028387\n",
      "iteration 1149 : loss : 0.132285, loss_ce: 0.035420\n",
      "iteration 1150 : loss : 0.124815, loss_ce: 0.051522\n",
      "iteration 1151 : loss : 0.146497, loss_ce: 0.032411\n",
      "iteration 1152 : loss : 0.124712, loss_ce: 0.046565\n",
      "iteration 1153 : loss : 0.157115, loss_ce: 0.023366\n",
      "iteration 1154 : loss : 0.123140, loss_ce: 0.041088\n",
      "iteration 1155 : loss : 0.146643, loss_ce: 0.040295\n",
      "iteration 1156 : loss : 0.126372, loss_ce: 0.035598\n",
      "iteration 1157 : loss : 0.180527, loss_ce: 0.048756\n",
      "iteration 1158 : loss : 0.127051, loss_ce: 0.044530\n",
      "iteration 1159 : loss : 0.117506, loss_ce: 0.032586\n",
      "iteration 1160 : loss : 0.160781, loss_ce: 0.052362\n",
      "iteration 1161 : loss : 0.152184, loss_ce: 0.021292\n",
      "iteration 1162 : loss : 0.153019, loss_ce: 0.029371\n",
      "iteration 1163 : loss : 0.135275, loss_ce: 0.041983\n",
      "iteration 1164 : loss : 0.101048, loss_ce: 0.034698\n",
      "iteration 1165 : loss : 0.161115, loss_ce: 0.047791\n",
      "iteration 1166 : loss : 0.140163, loss_ce: 0.046364\n",
      "iteration 1167 : loss : 0.133567, loss_ce: 0.036827\n",
      "iteration 1168 : loss : 0.110993, loss_ce: 0.034017\n",
      "iteration 1169 : loss : 0.149768, loss_ce: 0.020315\n",
      "iteration 1170 : loss : 0.128684, loss_ce: 0.035366\n",
      "iteration 1171 : loss : 0.206235, loss_ce: 0.032950\n",
      "iteration 1172 : loss : 0.180310, loss_ce: 0.028270\n",
      "iteration 1173 : loss : 0.209285, loss_ce: 0.010364\n",
      "iteration 1174 : loss : 0.122917, loss_ce: 0.033552\n",
      "iteration 1175 : loss : 0.145379, loss_ce: 0.051825\n",
      "iteration 1176 : loss : 0.194480, loss_ce: 0.033207\n",
      "iteration 1177 : loss : 0.098391, loss_ce: 0.028930\n",
      "iteration 1178 : loss : 0.131688, loss_ce: 0.041806\n",
      "iteration 1179 : loss : 0.177158, loss_ce: 0.046102\n",
      "iteration 1180 : loss : 0.132650, loss_ce: 0.039690\n",
      "iteration 1181 : loss : 0.150651, loss_ce: 0.045952\n",
      "iteration 1182 : loss : 0.116063, loss_ce: 0.038355\n",
      "iteration 1183 : loss : 0.151487, loss_ce: 0.037255\n",
      "iteration 1184 : loss : 0.252767, loss_ce: 0.019224\n",
      "iteration 1185 : loss : 0.139002, loss_ce: 0.046756\n",
      "iteration 1186 : loss : 0.156308, loss_ce: 0.053135\n",
      "iteration 1187 : loss : 0.116732, loss_ce: 0.033559\n",
      "iteration 1188 : loss : 0.198427, loss_ce: 0.047288\n",
      "iteration 1189 : loss : 0.149386, loss_ce: 0.044567\n",
      "iteration 1190 : loss : 0.130149, loss_ce: 0.049399\n",
      "iteration 1191 : loss : 0.110905, loss_ce: 0.027760\n",
      "iteration 1192 : loss : 0.164012, loss_ce: 0.040194\n",
      "iteration 1193 : loss : 0.124643, loss_ce: 0.033739\n",
      "iteration 1194 : loss : 0.123323, loss_ce: 0.057508\n",
      "iteration 1195 : loss : 0.156528, loss_ce: 0.050497\n",
      "iteration 1196 : loss : 0.129436, loss_ce: 0.032660\n",
      "iteration 1197 : loss : 0.135278, loss_ce: 0.039588\n",
      "iteration 1198 : loss : 0.147383, loss_ce: 0.050637\n",
      "iteration 1199 : loss : 0.127482, loss_ce: 0.027069\n",
      "iteration 1200 : loss : 0.191564, loss_ce: 0.029492\n",
      "iteration 1201 : loss : 0.140930, loss_ce: 0.058872\n",
      "iteration 1202 : loss : 0.136190, loss_ce: 0.048908\n",
      "iteration 1203 : loss : 0.141187, loss_ce: 0.038332\n",
      "iteration 1204 : loss : 0.133067, loss_ce: 0.046486\n",
      "iteration 1205 : loss : 0.184110, loss_ce: 0.040356\n",
      "iteration 1206 : loss : 0.134272, loss_ce: 0.057143\n",
      "iteration 1207 : loss : 0.144349, loss_ce: 0.032142\n",
      "iteration 1208 : loss : 0.192580, loss_ce: 0.056550\n",
      "iteration 1209 : loss : 0.357522, loss_ce: 0.013260\n",
      " 26%|████████▌                        | 13/50 [12:39<35:54, 58.24s/it]iteration 1210 : loss : 0.118392, loss_ce: 0.040263\n",
      "iteration 1211 : loss : 0.165752, loss_ce: 0.033102\n",
      "iteration 1212 : loss : 0.107113, loss_ce: 0.028404\n",
      "iteration 1213 : loss : 0.159065, loss_ce: 0.051029\n",
      "iteration 1214 : loss : 0.151522, loss_ce: 0.054053\n",
      "iteration 1215 : loss : 0.091991, loss_ce: 0.023184\n",
      "iteration 1216 : loss : 0.159478, loss_ce: 0.054258\n",
      "iteration 1217 : loss : 0.094235, loss_ce: 0.030549\n",
      "iteration 1218 : loss : 0.110839, loss_ce: 0.027651\n",
      "iteration 1219 : loss : 0.152178, loss_ce: 0.036130\n",
      "iteration 1220 : loss : 0.117517, loss_ce: 0.051912\n",
      "iteration 1221 : loss : 0.159195, loss_ce: 0.030998\n",
      "iteration 1222 : loss : 0.128108, loss_ce: 0.028294\n",
      "iteration 1223 : loss : 0.118648, loss_ce: 0.038226\n",
      "iteration 1224 : loss : 0.143616, loss_ce: 0.054544\n",
      "iteration 1225 : loss : 0.102310, loss_ce: 0.024985\n",
      "iteration 1226 : loss : 0.117766, loss_ce: 0.045877\n",
      "iteration 1227 : loss : 0.118150, loss_ce: 0.028209\n",
      "iteration 1228 : loss : 0.111497, loss_ce: 0.032689\n",
      "iteration 1229 : loss : 0.120348, loss_ce: 0.021231\n",
      "iteration 1230 : loss : 0.123191, loss_ce: 0.040782\n",
      "iteration 1231 : loss : 0.131693, loss_ce: 0.037669\n",
      "iteration 1232 : loss : 0.175277, loss_ce: 0.033907\n",
      "iteration 1233 : loss : 0.140254, loss_ce: 0.020871\n",
      "iteration 1234 : loss : 0.139888, loss_ce: 0.052207\n",
      "iteration 1235 : loss : 0.100830, loss_ce: 0.028167\n",
      "iteration 1236 : loss : 0.157089, loss_ce: 0.021632\n",
      "iteration 1237 : loss : 0.180433, loss_ce: 0.023411\n",
      "iteration 1238 : loss : 0.167868, loss_ce: 0.035597\n",
      "iteration 1239 : loss : 0.178452, loss_ce: 0.042006\n",
      "iteration 1240 : loss : 0.125631, loss_ce: 0.053041\n",
      "iteration 1241 : loss : 0.188599, loss_ce: 0.043195\n",
      "iteration 1242 : loss : 0.137982, loss_ce: 0.039252\n",
      "iteration 1243 : loss : 0.144729, loss_ce: 0.054247\n",
      "iteration 1244 : loss : 0.139860, loss_ce: 0.030798\n",
      "iteration 1245 : loss : 0.103171, loss_ce: 0.045038\n",
      "iteration 1246 : loss : 0.159706, loss_ce: 0.030607\n",
      "iteration 1247 : loss : 0.151360, loss_ce: 0.032733\n",
      "iteration 1248 : loss : 0.148306, loss_ce: 0.035736\n",
      "iteration 1249 : loss : 0.119512, loss_ce: 0.051326\n",
      "iteration 1250 : loss : 0.123415, loss_ce: 0.045148\n",
      "iteration 1251 : loss : 0.127562, loss_ce: 0.027840\n",
      "iteration 1252 : loss : 0.134764, loss_ce: 0.050263\n",
      "iteration 1253 : loss : 0.134454, loss_ce: 0.046128\n",
      "iteration 1254 : loss : 0.112412, loss_ce: 0.030423\n",
      "iteration 1255 : loss : 0.149239, loss_ce: 0.014615\n",
      "iteration 1256 : loss : 0.105342, loss_ce: 0.031411\n",
      "iteration 1257 : loss : 0.137924, loss_ce: 0.035504\n",
      "iteration 1258 : loss : 0.126554, loss_ce: 0.051929\n",
      "iteration 1259 : loss : 0.175968, loss_ce: 0.035444\n",
      "iteration 1260 : loss : 0.126781, loss_ce: 0.042105\n",
      "iteration 1261 : loss : 0.100642, loss_ce: 0.024463\n",
      "iteration 1262 : loss : 0.107790, loss_ce: 0.040184\n",
      "iteration 1263 : loss : 0.122371, loss_ce: 0.041330\n",
      "iteration 1264 : loss : 0.130831, loss_ce: 0.025977\n",
      "iteration 1265 : loss : 0.120393, loss_ce: 0.053056\n",
      "iteration 1266 : loss : 0.083808, loss_ce: 0.025177\n",
      "iteration 1267 : loss : 0.118050, loss_ce: 0.031513\n",
      "iteration 1268 : loss : 0.226157, loss_ce: 0.034488\n",
      "iteration 1269 : loss : 0.129771, loss_ce: 0.025128\n",
      "iteration 1270 : loss : 0.122731, loss_ce: 0.022280\n",
      "iteration 1271 : loss : 0.119051, loss_ce: 0.022255\n",
      "iteration 1272 : loss : 0.145166, loss_ce: 0.024087\n",
      "iteration 1273 : loss : 0.161714, loss_ce: 0.030627\n",
      "iteration 1274 : loss : 0.186207, loss_ce: 0.035122\n",
      "iteration 1275 : loss : 0.167134, loss_ce: 0.026868\n",
      "iteration 1276 : loss : 0.138180, loss_ce: 0.045464\n",
      "iteration 1277 : loss : 0.133924, loss_ce: 0.038999\n",
      "iteration 1278 : loss : 0.252134, loss_ce: 0.031130\n",
      "iteration 1279 : loss : 0.150341, loss_ce: 0.036957\n",
      "iteration 1280 : loss : 0.161078, loss_ce: 0.041963\n",
      "iteration 1281 : loss : 0.110021, loss_ce: 0.017547\n",
      "iteration 1282 : loss : 0.143531, loss_ce: 0.035610\n",
      "iteration 1283 : loss : 0.127321, loss_ce: 0.041389\n",
      "iteration 1284 : loss : 0.174869, loss_ce: 0.067402\n",
      "iteration 1285 : loss : 0.126036, loss_ce: 0.041612\n",
      "iteration 1286 : loss : 0.148271, loss_ce: 0.053427\n",
      "iteration 1287 : loss : 0.097570, loss_ce: 0.024618\n",
      "iteration 1288 : loss : 0.128027, loss_ce: 0.034793\n",
      "iteration 1289 : loss : 0.130209, loss_ce: 0.032313\n",
      "iteration 1290 : loss : 0.102395, loss_ce: 0.031459\n",
      "iteration 1291 : loss : 0.110604, loss_ce: 0.030131\n",
      "iteration 1292 : loss : 0.142802, loss_ce: 0.059570\n",
      "iteration 1293 : loss : 0.163383, loss_ce: 0.061782\n",
      "iteration 1294 : loss : 0.105722, loss_ce: 0.034112\n",
      "iteration 1295 : loss : 0.183991, loss_ce: 0.049044\n",
      "iteration 1296 : loss : 0.129576, loss_ce: 0.038889\n",
      "iteration 1297 : loss : 0.126960, loss_ce: 0.037789\n",
      "iteration 1298 : loss : 0.124282, loss_ce: 0.050338\n",
      "iteration 1299 : loss : 0.159418, loss_ce: 0.034171\n",
      "iteration 1300 : loss : 0.107153, loss_ce: 0.028086\n",
      "iteration 1301 : loss : 0.125147, loss_ce: 0.028101\n",
      "iteration 1302 : loss : 0.324691, loss_ce: 0.009058\n",
      " 28%|█████████▏                       | 14/50 [13:37<34:44, 57.90s/it]iteration 1303 : loss : 0.148260, loss_ce: 0.048485\n",
      "iteration 1304 : loss : 0.202051, loss_ce: 0.032303\n",
      "iteration 1305 : loss : 0.104365, loss_ce: 0.037748\n",
      "iteration 1306 : loss : 0.111898, loss_ce: 0.035212\n",
      "iteration 1307 : loss : 0.143257, loss_ce: 0.032256\n",
      "iteration 1308 : loss : 0.119517, loss_ce: 0.025018\n",
      "iteration 1309 : loss : 0.149496, loss_ce: 0.026403\n",
      "iteration 1310 : loss : 0.095757, loss_ce: 0.026142\n",
      "iteration 1311 : loss : 0.127047, loss_ce: 0.054799\n",
      "iteration 1312 : loss : 0.147849, loss_ce: 0.063976\n",
      "iteration 1313 : loss : 0.205592, loss_ce: 0.026548\n",
      "iteration 1314 : loss : 0.121427, loss_ce: 0.036973\n",
      "iteration 1315 : loss : 0.156180, loss_ce: 0.045381\n",
      "iteration 1316 : loss : 0.132274, loss_ce: 0.023333\n",
      "iteration 1317 : loss : 0.189592, loss_ce: 0.025104\n",
      "iteration 1318 : loss : 0.110967, loss_ce: 0.028999\n",
      "iteration 1319 : loss : 0.107395, loss_ce: 0.042225\n",
      "iteration 1320 : loss : 0.114173, loss_ce: 0.028777\n",
      "iteration 1321 : loss : 0.094921, loss_ce: 0.032834\n",
      "iteration 1322 : loss : 0.087191, loss_ce: 0.026756\n",
      "iteration 1323 : loss : 0.140723, loss_ce: 0.038169\n",
      "iteration 1324 : loss : 0.096614, loss_ce: 0.024521\n",
      "iteration 1325 : loss : 0.116134, loss_ce: 0.041066\n",
      "iteration 1326 : loss : 0.113370, loss_ce: 0.039494\n",
      "iteration 1327 : loss : 0.133724, loss_ce: 0.027406\n",
      "iteration 1328 : loss : 0.113048, loss_ce: 0.043266\n",
      "iteration 1329 : loss : 0.126247, loss_ce: 0.026700\n",
      "iteration 1330 : loss : 0.109474, loss_ce: 0.025903\n",
      "iteration 1331 : loss : 0.109865, loss_ce: 0.018619\n",
      "iteration 1332 : loss : 0.101781, loss_ce: 0.027887\n",
      "iteration 1333 : loss : 0.127365, loss_ce: 0.040019\n",
      "iteration 1334 : loss : 0.166985, loss_ce: 0.035771\n",
      "iteration 1335 : loss : 0.132774, loss_ce: 0.019582\n",
      "iteration 1336 : loss : 0.096195, loss_ce: 0.032292\n",
      "iteration 1337 : loss : 0.136900, loss_ce: 0.039697\n",
      "iteration 1338 : loss : 0.140259, loss_ce: 0.028826\n",
      "iteration 1339 : loss : 0.106792, loss_ce: 0.052014\n",
      "iteration 1340 : loss : 0.113369, loss_ce: 0.034042\n",
      "iteration 1341 : loss : 0.112768, loss_ce: 0.039412\n",
      "iteration 1342 : loss : 0.136997, loss_ce: 0.027478\n",
      "iteration 1343 : loss : 0.157920, loss_ce: 0.033210\n",
      "iteration 1344 : loss : 0.116685, loss_ce: 0.031134\n",
      "iteration 1345 : loss : 0.105042, loss_ce: 0.039781\n",
      "iteration 1346 : loss : 0.115827, loss_ce: 0.042937\n",
      "iteration 1347 : loss : 0.114376, loss_ce: 0.022720\n",
      "iteration 1348 : loss : 0.139202, loss_ce: 0.057696\n",
      "iteration 1349 : loss : 0.117680, loss_ce: 0.034229\n",
      "iteration 1350 : loss : 0.143598, loss_ce: 0.022902\n",
      "iteration 1351 : loss : 0.109075, loss_ce: 0.035488\n",
      "iteration 1352 : loss : 0.132403, loss_ce: 0.031513\n",
      "iteration 1353 : loss : 0.169608, loss_ce: 0.036195\n",
      "iteration 1354 : loss : 0.119804, loss_ce: 0.050211\n",
      "iteration 1355 : loss : 0.160989, loss_ce: 0.073467\n",
      "iteration 1356 : loss : 0.199145, loss_ce: 0.087259\n",
      "iteration 1357 : loss : 0.120296, loss_ce: 0.044083\n",
      "iteration 1358 : loss : 0.156347, loss_ce: 0.033231\n",
      "iteration 1359 : loss : 0.149114, loss_ce: 0.042704\n",
      "iteration 1360 : loss : 0.178875, loss_ce: 0.084033\n",
      "iteration 1361 : loss : 0.196708, loss_ce: 0.075981\n",
      "iteration 1362 : loss : 0.133692, loss_ce: 0.051595\n",
      "iteration 1363 : loss : 0.164462, loss_ce: 0.047794\n",
      "iteration 1364 : loss : 0.183431, loss_ce: 0.061506\n",
      "iteration 1365 : loss : 0.183160, loss_ce: 0.090906\n",
      "iteration 1366 : loss : 0.145751, loss_ce: 0.047487\n",
      "iteration 1367 : loss : 0.170218, loss_ce: 0.047079\n",
      "iteration 1368 : loss : 0.144352, loss_ce: 0.058454\n",
      "iteration 1369 : loss : 0.135886, loss_ce: 0.056126\n",
      "iteration 1370 : loss : 0.134113, loss_ce: 0.038640\n",
      "iteration 1371 : loss : 0.162805, loss_ce: 0.037272\n",
      "iteration 1372 : loss : 0.169694, loss_ce: 0.037983\n",
      "iteration 1373 : loss : 0.150275, loss_ce: 0.021282\n",
      "iteration 1374 : loss : 0.110816, loss_ce: 0.037731\n",
      "iteration 1375 : loss : 0.183284, loss_ce: 0.029812\n",
      "iteration 1376 : loss : 0.111540, loss_ce: 0.035814\n",
      "iteration 1377 : loss : 0.134460, loss_ce: 0.050661\n",
      "iteration 1378 : loss : 0.107518, loss_ce: 0.041783\n",
      "iteration 1379 : loss : 0.145222, loss_ce: 0.039619\n",
      "iteration 1380 : loss : 0.151575, loss_ce: 0.023227\n",
      "iteration 1381 : loss : 0.130009, loss_ce: 0.034821\n",
      "iteration 1382 : loss : 0.125760, loss_ce: 0.032604\n",
      "iteration 1383 : loss : 0.090780, loss_ce: 0.038775\n",
      "iteration 1384 : loss : 0.122122, loss_ce: 0.032635\n",
      "iteration 1385 : loss : 0.155966, loss_ce: 0.037898\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataset Synapse --cfg configs/swin_tiny_patch4_window7_224_lite.yaml --root_path ../data/Synapse --max_epochs 50 --output_dir output/  --img_size 224 --base_lr 0.05 --batch_size 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from configs/swin_tiny_patch4_window7_224_lite.yaml\n",
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:9\n",
      "---final upsample expand_first---\n",
      "self trained swin unet <All keys matched successfully>\n",
      "Namespace(Dataset=datasets.dataset_synapse.Synapse_dataset, accumulation_steps=None, amp_opt_level='O1', base_lr=0.05, batch_size=24, cache_mode='part', cfg='configs/swin_tiny_patch4_window7_224_lite.yaml', dataset='Synapse', deterministic=1, eval=False, img_size=224, is_pretrain=True, is_savenii=True, list_dir='./lists/lists_Synapse', max_epochs=3, max_iterations=30000, num_classes=9, opts=None, output_dir='output/', resume=None, seed=1234, tag=None, test_save_dir='../predictions', throughput=False, use_checkpoint=False, volume_path='../data/Synapse/test_vol_h5', z_spacing=1, zip=False)\n",
      "epoch_2.pth\n",
      "12 test iterations per epoch\n",
      "0it [00:00, ?it/s]idx 0 case case0008 mean_dice 0.119123 mean_hd95 88.990286\n",
      "1it [01:57, 117.21s/it]idx 1 case case0022 mean_dice 0.151445 mean_hd95 110.291896\n",
      "2it [03:15, 94.60s/it] idx 2 case case0038 mean_dice 0.128861 mean_hd95 97.612211\n",
      "3it [04:36, 88.06s/it]idx 3 case case0036 mean_dice 0.104906 mean_hd95 159.482013\n",
      "4it [07:29, 121.75s/it]idx 4 case case0032 mean_dice 0.150467 mean_hd95 101.783532\n",
      "5it [09:27, 120.36s/it]idx 5 case case0002 mean_dice 0.170523 mean_hd95 82.917290\n",
      "6it [11:13, 115.49s/it]idx 6 case case0029 mean_dice 0.180208 mean_hd95 77.013454\n",
      "7it [12:29, 102.62s/it]idx 7 case case0003 mean_dice 0.153357 mean_hd95 114.655701\n",
      "8it [15:29, 127.05s/it]"
     ]
    }
   ],
   "source": [
    "!python test.py --dataset Synapse --cfg configs/swin_tiny_patch4_window7_224_lite.yaml --is_saveni --volume_path ../data/Synapse --output_dir output/ --max_epoch 150 --base_lr 0.05 --img_size 224 --batch_size 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40d270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
